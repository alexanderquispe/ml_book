[["penalized-linear-regressions-a-simulation-experiment.html", "Chapter 6 Penalized Linear Regressions: A Simulation Experiment 6.1 Data Generating Process: Approximately Sparse", " Chapter 6 Penalized Linear Regressions: A Simulation Experiment 6.1 Data Generating Process: Approximately Sparse Import libraries R code # No libraries requiered   Python code import random import numpy as np import math import matplotlib.pyplot as plt To set seed R code set.seed(1) n = 100 p = 400   Python code random.seed(1) n = 100 p = 400 To create variables Z= runif(n)-1/2 W = matrix(runif(n*p)-1/2, n, p)   Z = np.random.uniform( low = 0 , high = 1 , size = n) - 1/2 W = ( np.random.uniform( low = 0 , high = 1 , size = n * p ) - 1/2 ).\\ reshape( n , p ) beta = 1/seq(1:p)^2 # approximately sparse beta gX = exp(4*Z)+ W%*%beta # leading term nonlinear X = cbind(Z, Z^2, Z^3, W ) # polynomials in Zs will be approximating exp(4*Z)   beta = ((1/ np.arange(1, p + 1 )) ** 2) gX = np.exp( 4 * Z ) + (W @ beta) X = np.concatenate( ( Z.reshape(Z.size, 1), Z.reshape(Z.size, 1) \\ ** 2, Z.reshape(Z.size, 1) ** 3, W ) , axis = 1 ) Generate \\(Y\\) Y = gX + rnorm(n) #generate Y   mean = 0 sd = 1 Y = gX + np.random.normal( mean , sd, n ) # # # # # plot(gX,Y, xlab=&quot;g(X)&quot;, ylab=&quot;Y&quot;, title(&quot;Y vs g(X)&quot;))   fig = plt.figure() fig.suptitle(&#39;Y vs g(X)&#39;) ax = fig.add_subplot(111) plt.scatter( Y, gX) plt.xlabel(&#39;g(X)&#39;) plt.ylabel(&#39;Y&#39;) plt.show() print( c(&quot;theoretical R2:&quot;, var(gX)/var(Y))) ## [1] &quot;theoretical R2:&quot; &quot;0.826881788964026&quot; dim(X) ## [1] 100 403   print( f&quot;theoretical R2:, {np.var(gX) / np.var( Y )}&quot; ) ## theoretical R2:, 0.7704499894280572 X.shape ## (100, 403) We use package Glmnet to carry out predictions using cross-validated lasso, ridge, and elastic net. We should know that cv.glmnet function in r standarize X data by default. So, we have to standarize our data before the execution of sklearn package. The normalize parameter will help for this. However, the function cv.glamnet is also standarizing the Y variable and then unstadarize the coefficients from the regression. To do this with sklearn, we will standarize the Y variable before fitting with StandardScaler function. Finally, the r-function uses 10 folds by default so we will adjust our model to use \\(cv=10\\) ten folds. The parameter \\(l1_{ratio}\\) corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. Specifically, \\(l1_{ratio} = 1\\) is the lasso penalty. Currently, \\(l1_{ratio} &lt;= 0.01\\) is not reliable, unless you supply your own sequence of alpha. library(glmnet)   from sklearn.linear_model import LassoCV from sklearn.preprocessing import StandardScaler from sklearn.linear_model import RidgeCV, ElasticNetCV import statsmodels.api as sm fit.lasso.cv &lt;- cv.glmnet(X, Y, family=&quot;gaussian&quot;, alpha=1) # family gaussian means that we&#39;ll be using square loss fit.ridge &lt;- cv.glmnet(X, Y, family=&quot;gaussian&quot;, alpha=0) # family gaussian means that we&#39;ll be using square loss fit.elnet &lt;- cv.glmnet(X, Y, family=&quot;gaussian&quot;, alpha=.5) # family gaussian means that we&#39;ll be using square loss yhat.lasso.cv &lt;- predict(fit.lasso.cv, newx = X) # predictions yhat.ridge &lt;- predict(fit.ridge, newx = X) yhat.elnet &lt;- predict(fit.elnet, newx = X) MSE.lasso.cv &lt;- summary(lm((gX-yhat.lasso.cv)^2~1))$coef[1:2] # report MSE and standard error for MSE for approximating g(X) MSE.ridge &lt;- summary(lm((gX-yhat.ridge)^2~1))$coef[1:2] # report MSE and standard error for MSE for approximating g(X) MSE.elnet &lt;- summary(lm((gX-yhat.elnet)^2~1))$coef[1:2] # report MSE and standard error for MSE for approximating g(X)   # Reshaping Y variable Y_vec = Y.reshape( Y.size, 1) # Scalar distribution scaler = StandardScaler() scaler.fit( Y_vec ) ## StandardScaler() std_Y = scaler.transform( Y_vec ) # Regressions fit_lasso_cv = LassoCV(cv = 10 , random_state = 0 , normalize = True ).fit( X, std_Y ) ## C:\\Users\\alexa\\ANACON~1\\envs\\R-RETI~1\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). ## return f(*args, **kwargs) fit_ridge = ElasticNetCV( cv = 10 , normalize = True , random_state = 0 , l1_ratio = 0.0001 ).fit( X, std_Y ) ## C:\\Users\\alexa\\ANACON~1\\envs\\R-RETI~1\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). ## return f(*args, **kwargs) fit_elnet = ElasticNetCV( cv = 10 , normalize = True , random_state = 0 , l1_ratio = 0.5, max_iter = 100000 ).fit( X, std_Y ) # Predictions ## C:\\Users\\alexa\\ANACON~1\\envs\\R-RETI~1\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). ## return f(*args, **kwargs) yhat_lasso_cv = scaler.inverse_transform( fit_lasso_cv.predict( X ) ) yhat_ridge = scaler.inverse_transform( fit_ridge.predict( X ) ) yhat_elnet = scaler.inverse_transform( fit_elnet.predict( X ) ) MSE_lasso_cv = sm.OLS( ((gX - yhat_lasso_cv)**2 ) , np.ones( yhat_lasso_cv.shape ) ).fit().summary2().tables[1].round(3) MSE_ridge = sm.OLS( ((gX - yhat_ridge)**2 ) , np.ones( yhat_ridge.size ) ).fit().summary2().tables[1].round(3) MSE_elnet = sm.OLS( ((gX - yhat_elnet)**2 ) , np.ones( yhat_elnet.size ) ).fit().summary2().tables[1].round(3) # our coefficient of MSE_elnet are far from r output Here we compute the lasso and ols post lasso using plug-in choices for penalty levels, using package hdm. Rlasso functionality: it is searching the right set of regressors. This function was made for the case of p regressors and n observations where p &gt;&gt;&gt;&gt; n. It assumes that the error is i.i.d. The errors may be non-Gaussian or heteroscedastic. The post lasso function makes OLS with the selected T regressors. To select those parameters, they use \\(\\lambda\\) as variable to penalize Funny thing: the function rlasso was named like that because it is the rigorous Lasso. We find a Python code that tries to replicate the main function of hdm r-package. I was made by Max Huppertz. His library is this repository. Download its repository and copy this folder to your site-packages folder. In my case it is located here C:-packages . We need to install this package pip install multiprocess. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
