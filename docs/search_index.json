[["penalized-linear-regressions-a-simulation-experiment.html", "Chapter 6 Penalized Linear Regressions: A Simulation Experiment 6.1 Data Generating Process: Approximately Sparse 6.2 Data Generating Process: Approximately Sparse + Small Dense Part", " Chapter 6 Penalized Linear Regressions: A Simulation Experiment 6.1 Data Generating Process: Approximately Sparse Import libraries R code # No libraries requiered   Python code import random import numpy as np import math import matplotlib.pyplot as plt To set seed R code set.seed(1) n = 100 p = 400   Python code random.seed(1) n = 100 p = 400 To create variables Z= runif(n)-1/2 W = matrix(runif(n*p)-1/2, n, p)   Z = np.random.uniform( low = 0 , high = 1 , size = n) - 1/2 W = ( np.random.uniform( low = 0 , high = 1 , size = n * p ) - 1/2 ).\\ reshape( n , p ) beta = 1/seq(1:p)^2 # approximately sparse beta gX = exp(4*Z)+ W%*%beta # leading term nonlinear X = cbind(Z, Z^2, Z^3, W ) # polynomials in Zs will be approximating exp(4*Z)   beta = ((1/ np.arange(1, p + 1 )) ** 2) gX = np.exp( 4 * Z ) + (W @ beta) X = np.concatenate( ( Z.reshape(Z.size, 1), Z.reshape(Z.size, 1) \\ ** 2, Z.reshape(Z.size, 1) ** 3, W ) , axis = 1 ) Generate \\(Y\\) Y = gX + rnorm(n) #generate Y   mean = 0 sd = 1 Y = gX + np.random.normal( mean , sd, n ) # # # # # plot(gX,Y, xlab=&quot;g(X)&quot;, ylab=&quot;Y&quot;, title(&quot;Y vs g(X)&quot;))   fig = plt.figure() fig.suptitle(&#39;Y vs g(X)&#39;) ax = fig.add_subplot(111) plt.scatter( Y, gX) plt.xlabel(&#39;g(X)&#39;) plt.ylabel(&#39;Y&#39;) plt.show() print( c(&quot;theoretical R2:&quot;, var(gX)/var(Y))) ## [1] &quot;theoretical R2:&quot; &quot;0.826881788964026&quot; dim(X) ## [1] 100 403   print( f&quot;theoretical R2:, {np.var(gX) / np.var( Y )}&quot; ) ## theoretical R2:, 0.7829623990512068 X.shape ## (100, 403) We use package Glmnet to carry out predictions using cross-validated lasso, ridge, and elastic net. We should know that cv.glmnet function in r standarize X data by default. So, we have to standarize our data before the execution of sklearn package. The normalize parameter will help for this. However, the function cv.glamnet is also standarizing the Y variable and then unstadarize the coefficients from the regression. To do this with sklearn, we will standarize the Y variable before fitting with StandardScaler function. Finally, the r-function uses 10 folds by default so we will adjust our model to use \\(cv=10\\) ten folds. The parameter \\(l1_{ratio}\\) corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. Specifically, \\(l1_{ratio} = 1\\) is the lasso penalty. Currently, \\(l1_{ratio} &lt;= 0.01\\) is not reliable, unless you supply your own sequence of alpha. library(glmnet)   from sklearn.linear_model import LassoCV from sklearn.preprocessing import StandardScaler from sklearn.linear_model import RidgeCV, ElasticNetCV import statsmodels.api as sm fit.lasso.cv &lt;- cv.glmnet(X, Y, family=&quot;gaussian&quot;, alpha=1) # family gaussian means that we&#39;ll be using square loss fit.ridge &lt;- cv.glmnet(X, Y, family=&quot;gaussian&quot;, alpha=0) # family gaussian means that we&#39;ll be using square loss fit.elnet &lt;- cv.glmnet(X, Y, family=&quot;gaussian&quot;, alpha=.5) # family gaussian means that we&#39;ll be using square loss yhat.lasso.cv &lt;- predict(fit.lasso.cv, newx = X) # predictions yhat.ridge &lt;- predict(fit.ridge, newx = X) yhat.elnet &lt;- predict(fit.elnet, newx = X) MSE.lasso.cv &lt;- summary(lm((gX-yhat.lasso.cv)^2~1))$coef[1:2] # report MSE and standard error for MSE for approximating g(X) MSE.ridge &lt;- summary(lm((gX-yhat.ridge)^2~1))$coef[1:2] # report MSE and standard error for MSE for approximating g(X) MSE.elnet &lt;- summary(lm((gX-yhat.elnet)^2~1))$coef[1:2] # report MSE and standard error for MSE for approximating g(X)   # Reshaping Y variable Y_vec = Y.reshape( Y.size, 1) # Scalar distribution scaler = StandardScaler() scaler.fit( Y_vec ) ## StandardScaler() std_Y = scaler.transform( Y_vec ) # Regressions fit_lasso_cv = LassoCV(cv = 10 , random_state = 0 , normalize = True ).fit( X, std_Y ) ## C:\\Users\\alexa\\ANACON~1\\envs\\R-RETI~1\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). ## return f(*args, **kwargs) fit_ridge = ElasticNetCV( cv = 10 , normalize = True , random_state = 0 , l1_ratio = 0.0001 ).fit( X, std_Y ) ## C:\\Users\\alexa\\ANACON~1\\envs\\R-RETI~1\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). ## return f(*args, **kwargs) fit_elnet = ElasticNetCV( cv = 10 , normalize = True , random_state = 0 , l1_ratio = 0.5, max_iter = 100000 ).fit( X, std_Y ) # Predictions ## C:\\Users\\alexa\\ANACON~1\\envs\\R-RETI~1\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). ## return f(*args, **kwargs) yhat_lasso_cv = scaler.inverse_transform( fit_lasso_cv.predict( X ) ) yhat_ridge = scaler.inverse_transform( fit_ridge.predict( X ) ) yhat_elnet = scaler.inverse_transform( fit_elnet.predict( X ) ) MSE_lasso_cv = sm.OLS( ((gX - yhat_lasso_cv)**2 ) , np.ones( yhat_lasso_cv.shape ) ).fit().summary2().tables[1].round(3) MSE_ridge = sm.OLS( ((gX - yhat_ridge)**2 ) , np.ones( yhat_ridge.size ) ).fit().summary2().tables[1].round(3) MSE_elnet = sm.OLS( ((gX - yhat_elnet)**2 ) , np.ones( yhat_elnet.size ) ).fit().summary2().tables[1].round(3) # our coefficient of MSE_elnet are far from r output Here we compute the lasso and ols post lasso using plug-in choices for penalty levels, using package hdm. Rlasso functionality: it is searching the right set of regressors. This function was made for the case of p regressors and n observations where p &gt;&gt;&gt;&gt; n. It assumes that the error is i.i.d. The errors may be non-Gaussian or heteroscedastic. The post lasso function makes OLS with the selected T regressors. To select those parameters, they use \\(\\lambda\\) as variable to penalize Funny thing: the function rlasso was named like that because it is the rigorous Lasso. We find a Python code that tries to replicate the main function of hdm r-package. I was made by Max Huppertz. His library is this repository. Download its repository and copy this folder to your site-packages folder. In my case it is located here C:-packages . We need to install this package pip install multiprocess. R code library(hdm)   Python code import hdmpy fit.rlasso &lt;- rlasso(Y~X, post=FALSE) # lasso with plug-in penalty level fit.rlasso.post &lt;- rlasso(Y~X, post=TRUE) # post-lasso with plug-in penalty level yhat.rlasso &lt;- predict(fit.rlasso) #predict g(X) for values of X yhat.rlasso.post &lt;- predict(fit.rlasso.post) #predict g(X) for values of X MSE.lasso &lt;- summary(lm((gX-yhat.rlasso)^2~1))$coef[1:2] # report MSE and standard error for MSE for approximating g(X) MSE.lasso.post &lt;- summary(lm((gX-yhat.rlasso.post)^2~1))$coef[1:2] # report MSE and standard error for MSE for approximating g(X)   fit_rlasso = hdmpy.rlasso(X, Y, post = False) fit_rlasso_post = hdmpy.rlasso(X, Y, post = True) yhat_rlasso = Y - fit_rlasso.est[&#39;residuals&#39;].reshape( Y.size, ) yhat_rlasso_post = Y - fit_rlasso_post.est[&#39;residuals&#39;].reshape( Y.size , ) MSE_lasso = sm.OLS( ((gX - yhat_rlasso)**2 ) , np.ones( yhat_rlasso.size ) ).fit().summary2().tables[1].round(3) MSE_lasso_post = sm.OLS( ((gX - yhat_rlasso_post)**2 ) , np.ones( yhat_rlasso_post.size ) ).fit().summary2().tables[1].round(3) Next we code up lava, which alternates the fitting of lasso and ridge lava.predict&lt;- function(X,Y, iter=5){ g1 = predict(rlasso(X, Y, post=F)) #lasso step fits &quot;sparse part&quot; m1 = predict(glmnet(X, as.vector(Y-g1), family=&quot;gaussian&quot;, alpha=0, lambda =20),newx=X ) #ridge step fits the &quot;dense&quot; part i=1 while(i&lt;= iter) { g1 = predict(rlasso(X, Y, post=F)) #lasso step fits &quot;sparse part&quot; m1 = predict(glmnet(X, as.vector(Y-g1), family=&quot;gaussian&quot;, alpha=0, lambda =20),newx=X ); #ridge step fits the &quot;dense&quot; part i = i+1 } return(g1+m1); }   def lava_predict( x, y, iteration = 5 ): g1_rlasso = hdmpy.rlasso( x, y , post = False ) g1 = y - g1_rlasso.est[&#39;residuals&#39;].reshape( g1_rlasso.est[&#39;residuals&#39;].size, ) new_dep_var = y-g1 new_dep_var_vec = new_dep_var.reshape( new_dep_var.size, 1 ) # Scalar distribution scaler = StandardScaler() scaler.fit( new_dep_var_vec ) std_new_dep_var_vec = scaler.transform( new_dep_var_vec ) fit_ridge_m1 = ElasticNetCV( cv = 10 , normalize = True , random_state = 0 , l1_ratio = 0.0001, alphas = np.array([20]) ).fit( x, std_new_dep_var_vec ) m1 = scaler.inverse_transform( fit_ridge_m1.predict( x ) ) i = 1 while i &lt;= iteration: g1_rlasso = hdmpy.rlasso( x, y , post = False ) g1 = y - g1_rlasso.est[&#39;residuals&#39;].reshape( g1_rlasso.est[&#39;residuals&#39;].size, ) new_dep_var = y-g1 new_dep_var_vec = new_dep_var.reshape( new_dep_var.size, 1 ) # Scalar distribution scaler = StandardScaler() scaler.fit( new_dep_var_vec ) std_new_dep_var_vec = scaler.transform( new_dep_var_vec ) fit_ridge_m1 = ElasticNetCV( cv = 10 , normalize = True , random_state = 0 , l1_ratio = 0.0001, alphas = np.array([20]) ).fit( x, std_new_dep_var_vec ) m1 = scaler.inverse_transform( fit_ridge_m1.predict( x ) ) i = i + 1 return ( g1 + m1 ) yhat.lava = lava.predict(X,Y) MSE.lava &lt;- summary(lm((gX-yhat.lava)^2~1))$coef[1:2]# report MSE and standard error for MSE for approximating g(X) MSE.lava ## [1] 0.16013551 0.02531404   yhat_lava = lava_predict( X, Y ) ## C:\\Users\\alexa\\ANACON~1\\envs\\R-RETI~1\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). ## return f(*args, **kwargs) ## C:\\Users\\alexa\\ANACON~1\\envs\\R-RETI~1\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). ## return f(*args, **kwargs) ## C:\\Users\\alexa\\ANACON~1\\envs\\R-RETI~1\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). ## return f(*args, **kwargs) ## C:\\Users\\alexa\\ANACON~1\\envs\\R-RETI~1\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). ## return f(*args, **kwargs) ## C:\\Users\\alexa\\ANACON~1\\envs\\R-RETI~1\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). ## return f(*args, **kwargs) ## C:\\Users\\alexa\\ANACON~1\\envs\\R-RETI~1\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). ## return f(*args, **kwargs) MSE_lava = sm.OLS( ((gX - yhat_lava)**2 ) , np.ones( yhat_lava.size ) ).fit().summary2().tables[1].round(3) MSE_lava ## Coef. Std.Err. t P&gt;|t| [0.025 0.975] ## const 0.285 0.035 8.097 0.0 0.215 0.355 library(xtable) table&lt;- matrix(0, 6, 2) table[1,1:2] &lt;- MSE.lasso.cv table[2,1:2] &lt;- MSE.ridge table[3,1:2] &lt;- MSE.elnet table[4,1:2] &lt;- MSE.lasso table[5,1:2] &lt;- MSE.lasso.post table[6,1:2] &lt;- MSE.lava colnames(table)&lt;- c(&quot;MSA&quot;, &quot;S.E. for MSA&quot;) rownames(table)&lt;- c(&quot;Cross-Validated Lasso&quot;, &quot;Cross-Validated ridge&quot;,&quot;Cross-Validated elnet&quot;, &quot;Lasso&quot;,&quot;Post-Lasso&quot;,&quot;Lava&quot;) tab &lt;- xtable(table, digits =3) print(tab,type=&quot;latex&quot;) ## % latex table generated in R 4.1.1 by xtable 1.8-4 package ## % Tue Sep 28 23:02:10 2021 ## \\begin{table}[ht] ## \\centering ## \\begin{tabular}{rrr} ## \\hline ## &amp; MSA &amp; S.E. for MSA \\\\ ## \\hline ## Cross-Validated Lasso &amp; 0.367 &amp; 0.061 \\\\ ## Cross-Validated ridge &amp; 2.384 &amp; 0.370 \\\\ ## Cross-Validated elnet &amp; 0.399 &amp; 0.064 \\\\ ## Lasso &amp; 0.148 &amp; 0.027 \\\\ ## Post-Lasso &amp; 0.085 &amp; 0.009 \\\\ ## Lava &amp; 0.160 &amp; 0.025 \\\\ ## \\hline ## \\end{tabular} ## \\end{table}   import pandas as pd table2 = np.zeros( (6, 2) ) table2[0, 0:] = MSE_lasso_cv.iloc[0, 0:2].to_list() table2[1, 0:] = MSE_ridge.iloc[0, 0:2].to_list() table2[2, 0:] = MSE_elnet.iloc[0, 0:2].to_list() table2[3, 0:] = MSE_lasso.iloc[0, 0:2].to_list() table2[4, 0:] = MSE_lasso_post.iloc[0, 0:2].to_list() table2[5, 0:] = MSE_lava.iloc[0, 0:2].to_list() table2_pandas = pd.DataFrame( table2, columns = [ &quot;MSA&quot;,&quot;S.E. for MSA&quot; ]) table2_pandas.index = [ &quot;Cross-Validated Lasso&quot;,\\ &quot;Cross-Validated Ridge&quot;, &quot;Cross-Validated elnet&quot;,\\ &quot;Lasso&quot;, &quot;Post-Lasso&quot;, &quot;Lava&quot; ] table2_pandas = table2_pandas.round(3) table2_html = table2_pandas.to_html() table2_pandas ## MSA S.E. for MSA ## Cross-Validated Lasso 0.275 0.034 ## Cross-Validated Ridge 3.134 0.557 ## Cross-Validated elnet 0.532 0.070 ## Lasso 0.285 0.035 ## Post-Lasso 0.159 0.020 ## Lava 0.285 0.035 plot(gX, gX, pch=19, cex=1, ylab=&quot;predicted value&quot;, xlab=&quot;true g(X)&quot;) points(gX, yhat.rlasso, col=2, pch=18, cex = 1.5 ) points(gX, yhat.rlasso.post, col=3, pch=17, cex = 1.2 ) points( gX, yhat.lasso.cv,col=4, pch=19, cex = 1.2 ) legend(&quot;bottomright&quot;, legend = c(&quot;rLasso&quot;, &quot;Post-rLasso&quot;, &quot;CV Lasso&quot;),col = c(2,3,4), pch = c(18,17, 19),bty = &quot;n&quot;, pt.cex = 1.3, cex = 1.2, text.col = &quot;black&quot;, horiz = F ,inset = c(0.1, 0.1))   import matplotlib.pyplot as plt fig = plt.figure() ax1 = fig.add_subplot(111) ax1.scatter( gX, gX , marker = &#39;.&#39;, c = &#39;black&#39; ) ax1.scatter( gX, yhat_rlasso , marker = &#39;D&#39; , c = &#39;red&#39; , label = &#39;rLasso&#39; ) ax1.scatter( gX, yhat_rlasso_post , marker = &#39;^&#39; , c = &#39;green&#39; , label = &#39;Post-rLasso&#39;) ax1.scatter( gX, yhat_lasso_cv , marker = &#39;o&#39; , c = &#39;blue&#39; , label = &#39;CV Lasso&#39;) plt.legend(loc=&#39;lower right&#39;) plt.show() 6.2 Data Generating Process: Approximately Sparse + Small Dense Part R code set.seed(1) n = 100 p = 400 Z= runif(n)-1/2 W = matrix(runif(n*p)-1/2, n, p) beta = rnorm(p)*.2 # dense beta gX = exp(4*Z)+ W%*%beta # leading term nonlinear X = cbind(Z, Z^2, Z^3, W ) # polynomials in Zs will be approximating exp(4*Z) Y = gX + rnorm(n) #generate Y   Python code n = 100 p = 400 Z = np.random.uniform( low = 0 , high = 1 , size = n) - 1/2 W = ( np.random.uniform( low = 0 , high = 1 , size = n * p ) - 1/2 ).\\ reshape( n , p ) mean = 0 sd = 1 beta = ((np.random.normal( mean , sd, p )) * 0.2) gX = np.exp( 4 * Z ) + (W @ beta) X = np.concatenate( ( Z.reshape(Z.size, 1), Z.reshape(Z.size, 1) \\ ** 2, Z.reshape(Z.size, 1) ** 3, W ) , axis = 1 ) random.seed(2) Y = gX + np.random.normal( mean , sd, n ) plot(gX,Y, xlab=&quot;g(X)&quot;, ylab=&quot;Y&quot;) #plot V vs g(X) print( c(&quot;theoretical R2:&quot;, var(gX)/var(Y))) ## [1] &quot;theoretical R2:&quot; &quot;0.696687990094227&quot; var(gX)/var(Y) ## [,1] ## [1,] 0.696688   # We use package Glmnet to carry out predictions using cross-validated lasso, ridge, and elastic net fig = plt.figure() fig.suptitle(&#39;Y vs g(X)&#39;) ax = fig.add_subplot(111) plt.scatter( Y, gX) plt.xlabel(&#39;g(X)&#39;) plt.ylabel(&#39;Y&#39;) plt.show() print( f&quot;theoretical R2:, {np.var(gX) / np.var( Y )}&quot; ) ## theoretical R2:, 0.7995059021172473 np.var(gX) / np.var( Y ) #theoretical R-square in the simulation example ## 0.7995059021172473 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
