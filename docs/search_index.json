[["index.html", "Machine Learning and Causal Inference Chapter 1 Predictive Inference 1 1.1 Data 1.2 Data Analysis 1.3 Prediction Question 1.4 Data Splitting", " Machine Learning and Causal Inference Alexander Quispe 2021-09-17 Chapter 1 Predictive Inference 1 In labor economics an important question is what determines the wage of workers. This is a causal question, but we could begin to investigate from a predictive perspective. In the following wage example, \\(Y\\) is the hourly wage of a worker and \\(X\\) is a vector of workers characteristics, e.g., education, experience, gender. Two main questions here are: How to use job-relevant characteristics, such as education and experience, to best predict wages? What is the difference in predicted wages between men and women with the same job-relevant characteristics? In this lab, we focus on the prediction question first. 1.1 Data The data set we consider is from the March Supplement of the U.S. Current Population Survey, year 2015. We select white non-hispanic individuals, aged 25 to 64 years, and working more than 35 hours per week during at least 50 weeks of the year. We exclude self-employed workers; individuals living in group quarters; individuals in the military, agricultural or private household sectors; individuals with inconsistent reports on earnings and employment status; individuals with allocated or missing information in any of the variables used in the analysis; and individuals with hourly wage below \\(3\\). The variable of interest \\(Y\\) is the hourly wage rate constructed as the ratio of the annual earnings to the total number of hours worked, which is constructed in turn as the product of number of weeks worked and the usual number of hours worked per week. In our analysis, we also focus on single (never married) workers. The final sample is of size \\(n = 5150\\). 1.2 Data Analysis 1.2.1 R and Python code Import relevant packages R code library(dplyr) library(kableExtra) Python code # import pandas as pd # import numpy as np # import pyreadr We start loading the data set. R code load(&quot;./data/wage2015_subsample_inference.Rdata&quot;) Python code #rdata_read = pyreadr.read_r(&quot;../data/wage2015_subsample_inference.Rdata&quot;) #data = rdata_read[ &#39;data&#39; ] #type(data) #data.shape The dimensions are 5150 rows and 20 columns. Lets have a look at the structure of the data. R code # Calculate the means and convert it into a dataframe table0 &lt;- data.frame(lapply(data,class))%&gt;% tidyr::gather(&quot;Variable&quot;,&quot;Type&quot;) # Table presentation table0 %&gt;% kable(&quot;markdown&quot;,caption = &quot;Type of the Variables&quot;) Table 1.1: Type of the Variables Variable Type wage numeric lwage numeric sex numeric shs numeric hsg numeric scl numeric clg numeric ad numeric mw numeric so numeric we numeric ne numeric exp1 numeric exp2 numeric exp3 numeric exp4 numeric occ factor occ2 factor ind factor ind2 factor Python code # data.info() # data.describe() Give structure to the variables We are constructing the output variable \\(Y\\) and the matrix \\(Z\\) which includes the characteristics of workers that are given in the data. R code # Calculate the log wage. Y &lt;- log(data$wage) # Number of observaciones n &lt;- length(Y) # Regressors Z &lt;- data[,c(&quot;wage&quot;,&quot;lwage&quot;)] # Number of regressors p &lt;- dim(Z)[2] Number of observation: 5150 Number of raw regressors:2 Python code ## Calculate the log wage. # Y = np.log2(data[&#39;wage&#39;]) ## Number of observaciones # n = len(Y) ## Regressors # z = data.loc[:, ~data.columns.isin([&#39;wage&#39;, &#39;lwage&#39;,&#39;Unnamed: 0&#39;])] ## Number of regressors # p = z.shape[1] # print(&quot;Number of observation:&quot;, n, &#39;\\n&#39;) # print( &quot;Number of raw regressors:&quot;, p) For the outcome variable wage and a subset of the raw regressors, we calculate the empirical mean to get familiar with the data. R code # Select the variables. Z_subset &lt;- data[,c(&quot;lwage&quot;,&quot;sex&quot;,&quot;shs&quot;,&quot;hsg&quot;,&quot;scl&quot;,&quot;clg&quot;,&quot;ad&quot;,&quot;mw&quot;,&quot;so&quot;,&quot;we&quot;,&quot;ne&quot;,&quot;exp1&quot;)] # Create table table1 &lt;- data.frame(as.numeric(lapply(Z_subset,mean))) %&gt;% mutate(Variables = c(&quot;Log Wage&quot;,&quot;Sex&quot;,&quot;Some High School&quot;,&quot;High School Graduate&quot;,&quot;Some College&quot;,&quot;College Graduate&quot;, &quot;Advanced Degree&quot;,&quot;Midwest&quot;,&quot;South&quot;,&quot;West&quot;,&quot;Northeast&quot;,&quot;Experience&quot;)) %&gt;% rename(`Sample Mean` = `as.numeric.lapply.Z_subset..mean..`) %&gt;% select(2,1) # HTML table table1 %&gt;% kable(&quot;markdown&quot;,caption = &quot;Descriptive Statistics&quot;) Table 1.2: Descriptive Statistics Variables Sample Mean Log Wage 2.9707867 Sex 0.4444660 Some High School 0.0233010 High School Graduate 0.2438835 Some College 0.2780583 College Graduate 0.3176699 Advanced Degree 0.1370874 Midwest 0.2596117 South 0.2965049 West 0.2161165 Northeast 0.2277670 Experience 13.7605825 Python code # Z_subset = data.loc[:, data.columns.isin([&quot;lwage&quot;,&quot;sex&quot;,&quot;shs&quot;,&quot;hsg&quot;,&quot;scl&quot;,&quot;clg&quot;,&quot;ad&quot;,&quot;mw&quot;,&quot;so&quot;,&quot;we&quot;,&quot;ne&quot;,&quot;exp1&quot;])] # table = Z_subset.mean(axis=0) # table # table = pd.DataFrame(data=table, columns={&quot;Sample mean&quot;:&quot;0&quot;} ) # table.index # index1 = list(table.index) # index2 = [&quot;Log Wage&quot;,&quot;Sex&quot;,&quot;Some High School&quot;,&quot;High School Graduate&quot;,\\ # &quot;Some College&quot;,&quot;College Graduate&quot;, &quot;Advanced Degree&quot;,&quot;Midwest&quot;,\\ # &quot;South&quot;,&quot;West&quot;,&quot;Northeast&quot;,&quot;Experience&quot;] # table = table.rename(index=dict(zip(index1,index2))) # table E.g., the share of female workers in our sample is ~44% (\\(sex=1\\) if female). Alternatively, we can also print the table as latex. R code print(table1, type=&quot;latex&quot;) ## Variables Sample Mean ## 1 Log Wage 2.97078670 ## 2 Sex 0.44446602 ## 3 Some High School 0.02330097 ## 4 High School Graduate 0.24388350 ## 5 Some College 0.27805825 ## 6 College Graduate 0.31766990 ## 7 Advanced Degree 0.13708738 ## 8 Midwest 0.25961165 ## 9 South 0.29650485 ## 10 West 0.21611650 ## 11 Northeast 0.22776699 ## 12 Experience 13.76058252 Python code # print(table.to_latex()) 1.3 Prediction Question Now, we will construct a prediction rule for hourly wage \\(Y\\) , which depends linearly on job-relevant characteristics \\(X\\): \\[Y = \\beta X + \\epsilon \\] Our goals are Predict wages using various characteristics of workers. Assess the predictive performance using the (adjusted) sample MSE, the (adjusted) sample \\(R^2\\) and the out-of-sample \\(MSE\\) and \\(R^2\\). We employ two different specifications for prediction: Basic Model: \\(X\\) consists of a set of raw regressors (e.g. gender, experience, education indicators, occupation and industry indicators, regional indicators). Flexible Model: \\(X\\) consists of all raw regressors from the basic model plus occupation and industry indicators, transformations (e.g.,\\(exp2\\) and \\(exp3\\)) and additional two-way interactions of polynomial in experience with other regressors. An example of a regressor created through a two-way interaction is experience times the indicator of having a college degree. Using the Flexible Model, enables us to approximate the real relationship by a more complex regression model and therefore to reduce the bias. The Flexible Model increases the range of potential shapes of the estimated regression function. In general, flexible models often deliver good prediction accuracy but give models which are harder to interpret. Now, let us fit both models to our data by running ordinary least squares (ols): 1.3.1 Basic Model: R code basic &lt;- lwage~ (sex + exp1 + shs + hsg+ scl + clg + mw + so + we+occ2+ind2) regbasic &lt;- lm(basic, data=data) regbasic # estimated coefficients ## ## Call: ## lm(formula = basic, data = data) ## ## Coefficients: ## (Intercept) sex exp1 shs hsg scl ## 3.722235 -0.072857 0.008568 -0.592798 -0.504337 -0.411994 ## clg mw so we occ22 occ23 ## -0.182216 -0.027541 -0.034454 0.017249 -0.076472 -0.034678 ## occ24 occ25 occ26 occ27 occ28 occ29 ## -0.096202 -0.187915 -0.414933 -0.045987 -0.377847 -0.215752 ## occ210 occ211 occ212 occ213 occ214 occ215 ## -0.010623 -0.455834 -0.307589 -0.361440 -0.499495 -0.464482 ## occ216 occ217 occ218 occ219 occ220 occ221 ## -0.233715 -0.412588 -0.340418 -0.241480 -0.212628 -0.288413 ## occ222 ind23 ind24 ind25 ind26 ind27 ## -0.422394 -0.116836 -0.244493 -0.273533 -0.249368 -0.139588 ## ind28 ind29 ind210 ind211 ind212 ind213 ## -0.242948 -0.387485 -0.193851 -0.169063 -0.077436 -0.172604 ## ind214 ind215 ind216 ind217 ind218 ind219 ## -0.187005 -0.325364 -0.315399 -0.304405 -0.335386 -0.374121 ## ind220 ind221 ind222 ## -0.551932 -0.316679 -0.118971 Number of regressors in the basic model 51. Python Regression packages: # import statsmodels.api as sm # import statsmodels.formula.api as smf # basic = &#39;lwage ~ sex + exp1 + shs + hsg+ scl + clg + mw + so + we + occ2+ ind2&#39; # basic_results = smf.ols(basic , data=data).fit() # print(basic_results.summary()) # estimated coefficients # print( &quot;Number of regressors in the basic model:&quot;,len(basic_results.params), &#39;\\n&#39;) # number of regressors in the Basic Model 1.3.2 Flexible Model: R code flex &lt;- lwage ~ sex + shs+hsg+scl+clg+occ2+ind2+mw+so+we + (exp1+exp2+exp3+exp4)*(shs+hsg+scl+clg+occ2+ind2+mw+so+we) regflex &lt;- lm(flex, data=data) regflex # estimated coefficients ## ## Call: ## lm(formula = flex, data = data) ## ## Coefficients: ## (Intercept) sex shs hsg scl clg ## 3.8602606 -0.0695532 -0.1233089 -0.5289024 -0.2920581 -0.0411641 ## occ22 occ23 occ24 occ25 occ26 occ27 ## 0.1613397 0.2101514 0.0708570 -0.3960076 -0.2310611 0.3147249 ## occ28 occ29 occ210 occ211 occ212 occ213 ## -0.1875417 -0.3390270 0.0209545 -0.6424177 -0.0674774 -0.2329781 ## occ214 occ215 occ216 occ217 occ218 occ219 ## 0.2562009 -0.1938585 -0.0551256 -0.4156093 -0.4822168 -0.2579412 ## occ220 occ221 occ222 ind23 ind24 ind25 ## -0.3010203 -0.4271811 -0.8694527 -1.2473654 -0.0948281 -0.5293860 ## ind26 ind27 ind28 ind29 ind210 ind211 ## -0.6221688 -0.5047497 -0.7295442 -0.8025334 -0.5805840 -0.9852350 ## ind212 ind213 ind214 ind215 ind216 ind217 ## -0.7375777 -1.0183283 -0.5860174 -0.3801359 -0.5703905 -0.8201843 ## ind218 ind219 ind220 ind221 ind222 mw ## -0.7613604 -0.8812815 -0.9099021 -0.7586534 -0.4040775 0.1106834 ## so we exp1 exp2 exp3 exp4 ## 0.0224244 -0.0215659 -0.0677247 1.6362944 -0.9154735 0.1429357 ## shs:exp1 hsg:exp1 scl:exp1 clg:exp1 occ22:exp1 occ23:exp1 ## -0.1919981 -0.0173433 -0.0664505 -0.0550346 -0.0736239 -0.0714859 ## occ24:exp1 occ25:exp1 occ26:exp1 occ27:exp1 occ28:exp1 occ29:exp1 ## -0.0723997 0.0946732 -0.0348928 -0.2279338 -0.0727459 0.0274143 ## occ210:exp1 occ211:exp1 occ212:exp1 occ213:exp1 occ214:exp1 occ215:exp1 ## 0.0075628 0.1014221 -0.0862744 0.0067149 -0.1369153 -0.0400425 ## occ216:exp1 occ217:exp1 occ218:exp1 occ219:exp1 occ220:exp1 occ221:exp1 ## -0.0539314 0.0147277 0.1074099 0.0047165 0.0243156 0.0791776 ## occ222:exp1 ind23:exp1 ind24:exp1 ind25:exp1 ind26:exp1 ind27:exp1 ## 0.1093246 0.4758891 0.0147304 0.1256987 0.1540275 0.1029245 ## ind28:exp1 ind29:exp1 ind210:exp1 ind211:exp1 ind212:exp1 ind213:exp1 ## 0.2357669 0.1359079 0.1512578 0.3174885 0.2591089 0.3396094 ## ind214:exp1 ind215:exp1 ind216:exp1 ind217:exp1 ind218:exp1 ind219:exp1 ## 0.1441411 -0.0568181 0.0847295 0.1728867 0.1565399 0.1516103 ## ind220:exp1 ind221:exp1 ind222:exp1 mw:exp1 so:exp1 we:exp1 ## 0.1326629 0.2190905 0.1145814 -0.0279931 -0.0099678 0.0063077 ## shs:exp2 hsg:exp2 scl:exp2 clg:exp2 occ22:exp2 occ23:exp2 ## 1.9005060 0.1171642 0.6217923 0.4096746 0.6632173 0.6415456 ## occ24:exp2 occ25:exp2 occ26:exp2 occ27:exp2 occ28:exp2 occ29:exp2 ## 0.9748422 -0.9778823 0.1050860 3.1407119 0.6710877 0.0231977 ## occ210:exp2 occ211:exp2 occ212:exp2 occ213:exp2 occ214:exp2 occ215:exp2 ## -0.2692292 -1.0816539 0.8323737 -0.2209813 0.7511163 -0.0326858 ## occ216:exp2 occ217:exp2 occ218:exp2 occ219:exp2 occ220:exp2 occ221:exp2 ## 0.3635814 -0.2659285 -2.5608762 -0.1291756 -0.3323297 -0.9099997 ## occ222:exp2 ind23:exp2 ind24:exp2 ind25:exp2 ind26:exp2 ind27:exp2 ## -0.8550536 -5.9368948 -1.1053411 -2.0149181 -2.2277748 -1.4648099 ## ind28:exp2 ind29:exp2 ind210:exp2 ind211:exp2 ind212:exp2 ind213:exp2 ## -2.9479949 -1.7796219 -2.1973300 -3.8776807 -3.1690425 -3.9651983 ## ind214:exp2 ind215:exp2 ind216:exp2 ind217:exp2 ind218:exp2 ind219:exp2 ## -2.0783289 0.1911692 -1.3265850 -2.2002873 -2.2006232 -1.9308536 ## ind220:exp2 ind221:exp2 ind222:exp2 mw:exp2 so:exp2 we:exp2 ## -1.9467267 -3.1127363 -1.8578340 0.2005611 0.0544354 0.0012717 ## shs:exp3 hsg:exp3 scl:exp3 clg:exp3 occ22:exp3 occ23:exp3 ## -0.6721239 -0.0179937 -0.1997877 -0.1025230 -0.2039403 -0.2369620 ## occ24:exp3 occ25:exp3 occ26:exp3 occ27:exp3 occ28:exp3 occ29:exp3 ## -0.4366958 0.3885298 0.0484737 -1.3949288 -0.2053899 -0.0909660 ## occ210:exp3 occ211:exp3 occ212:exp3 occ213:exp3 occ214:exp3 occ215:exp3 ## 0.1854753 0.3931553 -0.2202559 0.0950356 -0.1443933 0.1477077 ## occ216:exp3 occ217:exp3 occ218:exp3 occ219:exp3 occ220:exp3 occ221:exp3 ## -0.0378548 0.1510497 1.4084443 0.0923425 0.1806994 0.3779083 ## occ222:exp3 ind23:exp3 ind24:exp3 ind25:exp3 ind26:exp3 ind27:exp3 ## 0.2855058 2.6665808 0.7298431 0.9942250 1.0641428 0.7089089 ## ind28:exp3 ind29:exp3 ind210:exp3 ind211:exp3 ind212:exp3 ind213:exp3 ## 1.2340948 0.8287315 1.0448162 1.6877578 1.3734455 1.6376669 ## ind214:exp3 ind215:exp3 ind216:exp3 ind217:exp3 ind218:exp3 ind219:exp3 ## 1.0162910 0.1879483 0.6889680 1.0085540 1.0605598 0.8959865 ## ind220:exp3 ind221:exp3 ind222:exp3 mw:exp3 so:exp3 we:exp3 ## 0.9768944 1.4415215 0.9687884 -0.0625771 -0.0115842 -0.0124875 ## shs:exp4 hsg:exp4 scl:exp4 clg:exp4 occ22:exp4 occ23:exp4 ## 0.0777418 0.0004913 0.0210760 0.0078695 0.0176389 0.0303057 ## occ24:exp4 occ25:exp4 occ26:exp4 occ27:exp4 occ28:exp4 occ29:exp4 ## 0.0584146 -0.0515181 -0.0170182 0.1905353 0.0196522 0.0190014 ## occ210:exp4 occ211:exp4 occ212:exp4 occ213:exp4 occ214:exp4 occ215:exp4 ## -0.0333347 -0.0465914 0.0110212 -0.0136895 0.0055582 -0.0327444 ## occ216:exp4 occ217:exp4 occ218:exp4 occ219:exp4 occ220:exp4 occ221:exp4 ## -0.0089706 -0.0256735 -0.2121372 -0.0169398 -0.0296125 -0.0524577 ## occ222:exp4 ind23:exp4 ind24:exp4 ind25:exp4 ind26:exp4 ind27:exp4 ## -0.0350646 -0.3851791 -0.1209478 -0.1441045 -0.1526110 -0.1001993 ## ind28:exp4 ind29:exp4 ind210:exp4 ind211:exp4 ind212:exp4 ind213:exp4 ## -0.1609664 -0.1178080 -0.1482842 -0.2322961 -0.1872911 -0.2155617 ## ind214:exp4 ind215:exp4 ind216:exp4 ind217:exp4 ind218:exp4 ind219:exp4 ## -0.1483524 -0.0532195 -0.1044336 -0.1427349 -0.1546248 -0.1269592 ## ind220:exp4 ind221:exp4 ind222:exp4 mw:exp4 so:exp4 we:exp4 ## -0.1468554 -0.2032619 -0.1480951 0.0062439 0.0003145 0.0017685 Number of regressors in the flexible model:246. Python code # flex = &#39;lwage ~ sex + shs+hsg+scl+clg+occ2+ind2+mw+so+we + (exp1+exp2+exp3+exp4)*(shs+hsg+scl+clg+occ2+ind2+mw+so+we)&#39; # flex_results_0 = smf.ols(flex , data=data) # flex_results = smf.ols(flex , data=data).fit() # print(flex_results.summary()) # estimated coefficients # print( &quot;Number of regressors in the basic model:&quot;,len(flex_results.params), &#39;\\n&#39;) 1.3.3 Lasso Model: First, we import the essential libraries. R code library(hdm) Python code # from sklearn.linear_model import LassoCV # from sklearn import linear_model # from sklearn.preprocessing import PolynomialFeatures # from sklearn.metrics import mean_squared_error Then, we estimate the lasso model. R code flex &lt;- lwage ~ sex + shs+hsg+scl+clg+occ2+ind2+mw+so+we + (exp1+exp2+exp3+exp4)*(shs+hsg+scl+clg+occ2+ind2+mw+so+we) lassoreg&lt;- rlasso(flex, data=data) sumlasso&lt;- summary(lassoreg) ## ## Call: ## rlasso.formula(formula = flex, data = data) ## ## Post-Lasso Estimation: TRUE ## ## Total number of variables: 245 ## Number of selected variables: 24 ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.03159 -0.29132 -0.01137 0.28472 3.63651 ## ## Estimate ## (Intercept) 3.137 ## sex 0.000 ## shs -0.480 ## hsg -0.404 ## scl -0.306 ## clg 0.000 ## occ22 0.041 ## occ23 0.109 ## occ24 0.000 ## occ25 0.000 ## occ26 -0.327 ## occ27 0.000 ## occ28 -0.257 ## occ29 0.000 ## occ210 0.038 ## occ211 -0.398 ## occ212 0.000 ## occ213 -0.131 ## occ214 -0.189 ## occ215 -0.387 ## occ216 0.000 ## occ217 -0.280 ## occ218 0.000 ## occ219 0.000 ## occ220 0.000 ## occ221 0.000 ## occ222 -0.209 ## ind23 0.000 ## ind24 0.000 ## ind25 0.000 ## ind26 0.000 ## ind27 0.000 ## ind28 0.000 ## ind29 -0.169 ## ind210 0.000 ## ind211 0.000 ## ind212 0.193 ## ind213 0.000 ## ind214 0.076 ## ind215 0.000 ## ind216 0.000 ## ind217 0.000 ## ind218 0.000 ## ind219 0.000 ## ind220 -0.225 ## ind221 0.000 ## ind222 0.140 ## mw 0.000 ## so 0.000 ## we 0.000 ## exp1 0.009 ## exp2 0.000 ## exp3 0.000 ## exp4 0.000 ## shs:exp1 0.000 ## hsg:exp1 0.000 ## scl:exp1 0.000 ## clg:exp1 0.000 ## occ22:exp1 0.000 ## occ23:exp1 0.000 ## occ24:exp1 0.000 ## occ25:exp1 0.000 ## occ26:exp1 0.000 ## occ27:exp1 0.000 ## occ28:exp1 0.000 ## occ29:exp1 0.000 ## occ210:exp1 0.003 ## occ211:exp1 0.000 ## occ212:exp1 0.000 ## occ213:exp1 -0.011 ## occ214:exp1 -0.009 ## occ215:exp1 0.000 ## occ216:exp1 0.000 ## occ217:exp1 0.000 ## occ218:exp1 0.000 ## occ219:exp1 0.000 ## occ220:exp1 0.000 ## occ221:exp1 0.000 ## occ222:exp1 0.000 ## ind23:exp1 0.000 ## ind24:exp1 0.000 ## ind25:exp1 0.000 ## ind26:exp1 0.000 ## ind27:exp1 0.000 ## ind28:exp1 0.000 ## ind29:exp1 0.000 ## ind210:exp1 0.000 ## ind211:exp1 0.000 ## ind212:exp1 0.000 ## ind213:exp1 0.000 ## ind214:exp1 0.004 ## ind215:exp1 0.000 ## ind216:exp1 0.000 ## ind217:exp1 0.000 ## ind218:exp1 0.000 ## ind219:exp1 0.000 ## ind220:exp1 0.000 ## ind221:exp1 0.000 ## ind222:exp1 0.000 ## mw:exp1 0.000 ## so:exp1 0.000 ## we:exp1 0.000 ## shs:exp2 0.000 ## hsg:exp2 0.000 ## scl:exp2 0.000 ## clg:exp2 0.000 ## occ22:exp2 0.000 ## occ23:exp2 0.000 ## occ24:exp2 0.000 ## occ25:exp2 0.000 ## occ26:exp2 0.000 ## occ27:exp2 0.000 ## occ28:exp2 0.000 ## occ29:exp2 0.000 ## occ210:exp2 0.000 ## occ211:exp2 0.000 ## occ212:exp2 0.000 ## occ213:exp2 0.000 ## occ214:exp2 0.000 ## occ215:exp2 0.000 ## occ216:exp2 0.000 ## occ217:exp2 0.000 ## occ218:exp2 0.000 ## occ219:exp2 0.000 ## occ220:exp2 0.000 ## occ221:exp2 0.000 ## occ222:exp2 0.000 ## ind23:exp2 0.000 ## ind24:exp2 0.000 ## ind25:exp2 0.000 ## ind26:exp2 0.000 ## ind27:exp2 0.000 ## ind28:exp2 0.000 ## ind29:exp2 0.000 ## ind210:exp2 0.000 ## ind211:exp2 0.000 ## ind212:exp2 0.000 ## ind213:exp2 0.000 ## ind214:exp2 0.000 ## ind215:exp2 0.000 ## ind216:exp2 0.000 ## ind217:exp2 0.000 ## ind218:exp2 0.000 ## ind219:exp2 0.000 ## ind220:exp2 0.000 ## ind221:exp2 0.000 ## ind222:exp2 0.000 ## mw:exp2 0.000 ## so:exp2 0.000 ## we:exp2 0.000 ## shs:exp3 0.000 ## hsg:exp3 0.000 ## scl:exp3 0.000 ## clg:exp3 0.000 ## occ22:exp3 0.000 ## occ23:exp3 0.000 ## occ24:exp3 0.000 ## occ25:exp3 0.000 ## occ26:exp3 0.000 ## occ27:exp3 0.000 ## occ28:exp3 0.000 ## occ29:exp3 0.000 ## occ210:exp3 0.000 ## occ211:exp3 0.000 ## occ212:exp3 0.000 ## occ213:exp3 0.000 ## occ214:exp3 0.000 ## occ215:exp3 0.000 ## occ216:exp3 0.000 ## occ217:exp3 0.000 ## occ218:exp3 0.000 ## occ219:exp3 0.000 ## occ220:exp3 0.000 ## occ221:exp3 0.000 ## occ222:exp3 0.000 ## ind23:exp3 0.000 ## ind24:exp3 0.000 ## ind25:exp3 0.000 ## ind26:exp3 0.000 ## ind27:exp3 0.000 ## ind28:exp3 0.000 ## ind29:exp3 0.000 ## ind210:exp3 0.000 ## ind211:exp3 0.000 ## ind212:exp3 0.000 ## ind213:exp3 0.000 ## ind214:exp3 0.000 ## ind215:exp3 0.000 ## ind216:exp3 0.000 ## ind217:exp3 0.000 ## ind218:exp3 0.000 ## ind219:exp3 0.000 ## ind220:exp3 0.000 ## ind221:exp3 0.000 ## ind222:exp3 0.000 ## mw:exp3 0.000 ## so:exp3 0.000 ## we:exp3 0.000 ## shs:exp4 0.000 ## hsg:exp4 0.000 ## scl:exp4 0.000 ## clg:exp4 0.000 ## occ22:exp4 0.000 ## occ23:exp4 0.000 ## occ24:exp4 0.000 ## occ25:exp4 0.000 ## occ26:exp4 0.000 ## occ27:exp4 0.000 ## occ28:exp4 0.000 ## occ29:exp4 0.000 ## occ210:exp4 0.000 ## occ211:exp4 0.000 ## occ212:exp4 0.000 ## occ213:exp4 0.000 ## occ214:exp4 0.000 ## occ215:exp4 0.000 ## occ216:exp4 0.000 ## occ217:exp4 0.000 ## occ218:exp4 0.000 ## occ219:exp4 0.000 ## occ220:exp4 0.000 ## occ221:exp4 0.000 ## occ222:exp4 0.000 ## ind23:exp4 0.000 ## ind24:exp4 0.000 ## ind25:exp4 0.000 ## ind26:exp4 0.000 ## ind27:exp4 0.000 ## ind28:exp4 0.000 ## ind29:exp4 0.000 ## ind210:exp4 0.000 ## ind211:exp4 0.000 ## ind212:exp4 0.000 ## ind213:exp4 0.000 ## ind214:exp4 0.000 ## ind215:exp4 0.000 ## ind216:exp4 0.000 ## ind217:exp4 0.000 ## ind218:exp4 0.000 ## ind219:exp4 0.000 ## ind220:exp4 0.000 ## ind221:exp4 0.000 ## ind222:exp4 0.000 ## mw:exp4 0.000 ## so:exp4 0.000 ## we:exp4 0.000 ## ## Residual standard error: 0.4847 ## Multiple R-squared: 0.2779 ## Adjusted R-squared: 0.2745 ## Joint significance test: ## the sup score statistic for joint significance test is 89.09 with a p-value of 0.004 Python code # Get exogenous variables from flexible model # X = flex_results_0.exog # X.shape # # # Set endogenous variable # lwage = data[&quot;lwage&quot;] # lwage.shape # alpha=0.1 # # Set penalty value = 0.1 # #reg = linear_model.Lasso(alpha=0.1/np.log(len(lwage))) # reg = linear_model.Lasso(alpha = alpha) # # # LASSO regression for flexible model # reg.fit(X, lwage) # lwage_lasso_fitted = reg.fit(X, lwage).predict( X ) # # # coefficients # reg.coef_ # print(&#39;Lasso Regression: R^2 score&#39;, reg.score(X, lwage)) Now, we can evaluate the performance of both models based on the (adjusted) \\(R^2_{sample}\\) and the (adjusted) \\(MSE_{sample}\\): R-Squared \\((R^2)\\) R code # Summary from basic and flexible model. sumbasic &lt;- summary(regbasic) sumflex &lt;- summary(regflex) # R-squared from basic, flexible and lasso models R2.1 &lt;- sumbasic$r.squared R2.adj1 &lt;- sumbasic$adj.r.squared R2.2 &lt;- sumflex$r.squared R2.adj2 &lt;- sumflex$adj.r.squared R2.L &lt;- sumlasso$r.squared R2.adjL &lt;- sumlasso$adj.r.squared R-squared for the basic model: 0.3100465. Adjusted R-squared for the basic model: 0.3032809. R-squared for the flexible model: 0.3511099. Adjusted R-squared for the flexible model: 0.3186919. R-squared for the lasso with flexible model: 0.2778653. Adjusted R-squared for the flexible model: 0.2744836. Python # Assess the predictive performance # R2_1 = basic_results.rsquared # print(&quot;R-squared for the basic model: &quot;, R2_1, &quot;\\n&quot;) # R2_adj1 = basic_results.rsquared_adj # print(&quot;adjusted R-squared for the basic model: &quot;, R2_adj1, &quot;\\n&quot;) # # R2_2 = flex_results.rsquared # print(&quot;R-squared for the basic model: &quot;, R2_2, &quot;\\n&quot;) # R2_adj2 = flex_results.rsquared_adj # print(&quot;adjusted R-squared for the basic model: &quot;, R2_adj2, &quot;\\n&quot;) # R2_L = reg.score(flex_results_0.exog, lwage) # print(&quot;R-squared for LASSO: &quot;, R2_L, &quot;\\n&quot;) # R2_adjL = 1 - (1-R2_L)*(len(lwage)-1)/(len(lwage)-X.shape[1]-1) # print(&quot;adjusted R-squared for LASSO: &quot;, R2_adjL, &quot;\\n&quot;) Mean Squared Error \\(MSE\\) R code MSE1 &lt;- mean(sumbasic$res^2) p1 &lt;- sumbasic$df[1] # number of regressors MSE.adj1 &lt;- (n/(n-p1))*MSE1 MSE2 &lt;-mean(sumflex$res^2) p2 &lt;- sumflex$df[1] MSE.adj2 &lt;- (n/(n-p2))*MSE2 MSEL &lt;-mean(sumlasso$res^2) pL &lt;- length(sumlasso$coef) MSE.adjL &lt;- (n/(n-pL))*MSEL MSE for the basic model: 0.2244251 Adjusted MSE for the basic model: 0.2266697 MSE for the flexible model: 0.2110681 Adjusted MSE for the flexible model: 0.221656 MSE for the lasso flexible model: 0.2348928 Adjusted MSE for the lasso flexible model: 0.2466758 Python code # calculating the MSE # MSE1 = np.mean(basic_results.resid**2) # print(&quot;MSE for the basic model: &quot;, MSE1, &quot;\\n&quot;) # p1 = len(basic_results.params) # number of regressors # n = len(lwage) # MSE_adj1 = (n/(n-p1))*MSE1 # print(&quot;adjusted MSE for the basic model: &quot;, MSE_adj1, &quot;\\n&quot;) # # MSE2 = np.mean(flex_results.resid**2) # print(&quot;MSE for the flexible model: &quot;, MSE2, &quot;\\n&quot;) # p2 = len(flex_results.params) # number of regressors # n = len(lwage) # MSE_adj2 = (n/(n-p2))*MSE2 # print(&quot;adjusted MSE for the flexible model: &quot;, MSE_adj2, &quot;\\n&quot;) # # # MSEL = mean_squared_error(lwage, lwage_lasso_fitted) # print(&quot;MSE for the LASSO model: &quot;, MSEL, &quot;\\n&quot;) # pL = reg.coef_.shape[0] # number of regressors # n = len(lwage) # MSE_adjL = (n/(n-pL))*MSEL # print(&quot;adjusted MSE for LASSO model: &quot;, MSE_adjL, &quot;\\n&quot;) Latex presentation R code Models &lt;- c(&quot;Basic reg&quot;,&quot;Flexible reg&quot;,&quot;Lasso reg&quot;) p &lt;- c(p1,p2,pL) R_2 &lt;- c(R2.1,R2.2,R2.L) MSE &lt;- c(MSE1,MSE2,MSEL) R_2_adj &lt;- c(R2.adj1,R2.adj2,R2.adjL) MSE_adj &lt;- c(MSE.adj1,MSE.adj2,MSE.adjL) data.frame(Models,p,R_2,MSE,R_2_adj,MSE_adj) %&gt;% kable(&quot;markdown&quot;,caption = &quot;Descriptive Statistics&quot;) Table 1.3: Descriptive Statistics Models p R_2 MSE R_2_adj MSE_adj Basic reg 51 0.3100465 0.2244251 0.3032809 0.2266697 Flexible reg 246 0.3511099 0.2110681 0.3186919 0.2216560 Lasso reg 246 0.2778653 0.2348928 0.2744836 0.2466758 Python code # import array_to_latex as a2l # # table = np.zeros((3, 5)) # table[0,0:5] = [p1, R2_1, MSE1, R2_adj1, MSE_adj1] # table[1,0:5] = [p2, R2_2, MSE2, R2_adj2, MSE_adj2] # table[2,0:5] = [pL, R2_L, MSEL, R2_adjL, MSE_adjL] # table # table = pd.DataFrame(table, columns = [&quot;p&quot;,&quot;$R^2_{sample}$&quot;,&quot;$MSE_{sample}$&quot;,&quot;$R^2_{adjusted}$&quot;, &quot;$MSE_{adjusted}$&quot;], index = [&quot;basic reg&quot;,&quot;flexible reg&quot;, &quot;lasso flex&quot;]) # table Considering all measures above, the flexible model performs slightly better than the basic model. One procedure to circumvent this issue is to use data splitting that is described and applied in the following. 1.4 Data Splitting Measure the prediction quality of the two models via data splitting: Randomly split the data into one training sample and one testing sample. Here we just use a simple method (stratified splitting is a more sophisticated version of splitting that we can consider). Use the training sample for estimating the parameters of the Basic Model and the Flexible Model. Use the testing sample for evaluation. Predict the \\(wage\\) of every observation in the testing sample based on the estimated parameters in the training sample. Calculate the Mean Squared Prediction Error \\(MSE_{test}\\) based on the testing sample for both prediction models. R code # to make the results replicable (generating random numbers) set.seed(1) # draw (4/5)*n random numbers from 1 to n without replacing them random_2 &lt;- sample(1:n, floor(n*4/5)) # training sample train &lt;- data[random_2,] # testing sample test &lt;- data[-random_2,] Python code # # Import relevant packages for splitting data # import random # import math # # # Set Seed # # to make the results replicable (generating random numbers) # np.random.seed(0) # random = np.random.randint(0,n, size=math.floor(n)) # data[&quot;random&quot;] = random # random # the array does not change # data_2 = data.sort_values(by=[&#39;random&#39;]) # data_2.head() # Create training and testing sample # train = data_2[ : math.floor(n*4/5)] # training sample # test = data_2[ math.floor(n*4/5) : ] # testing sample # print(train.shape) # print(test.shape) The train data dimensions are 4120 rows and 20 columns. We estimate the parameters using the training data set. # basic model # estimating the parameters in the training sample regbasic &lt;- lm(basic, data=train) regbasic ## ## Call: ## lm(formula = basic, data = train) ## ## Coefficients: ## (Intercept) sex exp1 shs hsg scl ## 3.641716 -0.059065 0.008635 -0.570657 -0.508393 -0.405932 ## clg mw so we occ22 occ23 ## -0.178154 -0.044486 -0.051111 0.004004 -0.083481 -0.036320 ## occ24 occ25 occ26 occ27 occ28 occ29 ## -0.091457 -0.126383 -0.416548 -0.046615 -0.385957 -0.220534 ## occ210 occ211 occ212 occ213 occ214 occ215 ## -0.030423 -0.460487 -0.317680 -0.375180 -0.465495 -0.494731 ## occ216 occ217 occ218 occ219 occ220 occ221 ## -0.212026 -0.413355 -0.329054 -0.263839 -0.241109 -0.293004 ## occ222 ind23 ind24 ind25 ind26 ind27 ## -0.410747 -0.075902 -0.121251 -0.206172 -0.151398 -0.081742 ## ind28 ind29 ind210 ind211 ind212 ind213 ## -0.186304 -0.304987 -0.126226 -0.090402 0.028445 -0.116162 ## ind214 ind215 ind216 ind217 ind218 ind219 ## -0.105521 -0.227273 -0.223604 -0.208887 -0.256808 -0.266724 ## ind220 ind221 ind222 ## -0.459034 -0.226775 -0.047166 # # basic model # # estimating the parameters in the training sample # basic_results = smf.ols(basic , data=train).fit() # print(basic_results.summary()) Then predict using the parameters in the testing sample. # calculating the out-of-sample MSE trainregbasic &lt;- predict(regbasic, newdata=test) trainregbasic ## 12 44 71 84 129 149 191 221 ## 3.395851 2.734493 3.151807 2.506919 2.368192 3.173461 3.065637 3.043757 ## 248 264 281 368 464 467 496 540 ## 2.699975 2.398635 3.080034 2.542424 2.700733 2.973891 2.940246 2.796404 ## 546 576 629 687 705 765 769 809 ## 2.793806 2.907308 3.213768 3.222378 2.758879 2.772328 2.753855 3.005369 ## 938 945 952 960 1003 1057 1059 1065 ## 2.502601 2.941366 3.166449 2.846778 3.070972 2.629048 3.036431 2.881320 ## 1075 1076 1079 1101 1105 1143 1161 1195 ## 3.414267 3.504898 2.283221 2.564178 2.737425 2.761703 3.606691 2.556301 ## 1224 1271 1299 1342 1387 1407 1508 1568 ## 2.861144 2.749369 2.809817 3.680845 3.445996 2.621132 2.826924 2.877508 ## 1593 1646 1669 1693 1711 1715 1748 1749 ## 3.011676 2.775702 2.923326 2.811011 2.844195 3.037463 3.038273 2.898368 ## 1751 1763 1782 1862 1889 1896 1915 1916 ## 2.913956 3.447804 2.987807 3.308917 3.448159 3.032423 2.639366 2.710743 ## 1917 1930 1933 1939 1945 1946 1964 1978 ## 3.388704 3.639454 3.560323 3.261379 2.991757 3.476764 2.574404 3.409102 ## 1999 2020 2055 2122 2132 2135 2194 2215 ## 2.815364 2.804909 2.796349 3.052717 2.760399 2.834512 3.066740 3.066740 ## 2224 2278 2315 2330 2333 2362 2381 2465 ## 2.996740 3.497591 3.208588 3.172049 3.137275 2.977112 2.911219 2.936761 ## 2488 2490 2500 2502 2555 2566 2592 2593 ## 3.107351 2.515200 2.884473 2.834005 3.120641 3.248789 3.190732 2.996836 ## 2604 2635 2660 2746 2851 2879 2919 2972 ## 3.362746 3.364898 3.195416 2.850228 3.155803 2.618133 2.600422 3.536249 ## 2990 3005 3020 3021 3103 3117 3195 3269 ## 2.486495 2.965091 2.784081 2.659487 3.183660 3.126963 3.537607 3.375366 ## 3313 3357 3383 3428 3501 3531 3575 3605 ## 2.800045 3.108703 2.707201 3.030192 3.530804 3.056041 3.475381 2.868124 ## 3630 3634 3641 3738 3803 3810 3819 3824 ## 2.802191 2.785823 2.978776 2.885989 3.312690 3.053702 3.262039 2.792172 ## 3853 3855 3858 3867 3975 3996 4008 4012 ## 2.929577 2.837561 3.453031 2.637223 3.295419 3.283195 3.190034 3.255441 ## 4015 4021 4027 4034 4042 4046 4056 4057 ## 3.513137 2.909294 3.791820 3.371755 3.487255 3.382399 3.146749 3.341796 ## 4068 4078 4088 4095 4108 4109 4110 4111 ## 3.130255 3.083729 3.136610 3.373162 3.350788 3.342153 3.049343 3.701652 ## 4119 4161 4180 4189 4190 4311 4318 4416 ## 2.852830 3.050451 2.661820 2.655253 3.127026 3.167800 2.660548 3.436540 ## 4476 4492 4495 4585 4627 4658 4668 4735 ## 2.915678 3.440465 2.554050 3.351613 3.121649 2.702883 2.576908 2.956567 ## 4752 4801 4805 4806 4837 4862 4865 4869 ## 2.586882 3.185483 3.288241 3.562156 3.517912 3.426356 3.174408 2.709436 ## 4873 4981 5013 5021 5026 5034 5043 5069 ## 2.973516 2.988795 2.473201 2.692698 3.187168 3.096083 3.258672 2.824127 ## 5087 5115 5117 5145 5177 5187 5207 5210 ## 3.081478 2.752523 2.596189 3.173009 2.658040 2.897046 2.965355 2.732098 ## 5232 5288 5343 5345 5347 5348 5444 5455 ## 3.498372 3.170164 2.911181 3.308917 3.203620 2.998361 2.655253 2.806754 ## 5487 5548 5617 5654 5667 5699 5766 5779 ## 3.177714 2.600207 2.648163 2.536779 3.101771 3.257564 2.673927 3.191729 ## 5793 5798 5830 5889 5892 5902 5919 5946 ## 3.111683 2.895035 2.890652 2.694029 3.010846 3.283855 2.762490 3.055507 ## 6002 6005 6034 6096 6132 6152 6210 6236 ## 3.169177 3.295419 2.537143 2.886118 2.765682 3.264142 2.632311 2.698241 ## 6256 6281 6293 6310 6425 6431 6434 6457 ## 2.791589 2.867499 2.682244 2.712499 2.813238 3.020245 2.523731 3.154739 ## 6465 6478 6538 6539 6544 6575 6606 6668 ## 2.389111 3.040710 2.684084 2.731215 2.828932 2.867409 2.676274 3.154494 ## 6682 6695 6739 6800 6829 6841 6844 6886 ## 2.778237 2.808591 2.555936 3.097376 3.314938 2.790101 2.238735 3.018277 ## 6890 6929 6938 6971 6978 7019 7020 7062 ## 3.155272 2.556840 3.428714 2.846166 3.235418 2.742063 2.772506 2.615001 ## 7089 7158 7212 7221 7300 7313 7340 7347 ## 3.221539 2.731686 2.423232 2.611822 2.891218 2.688873 2.853702 2.904171 ## 7381 7389 7458 7508 7516 7560 7576 7609 ## 2.891335 2.551294 2.834250 2.462321 2.593496 2.901132 2.919222 3.151498 ## 7633 7646 7686 7710 7746 7773 7783 7793 ## 2.860321 2.264641 3.184857 3.467215 2.457641 3.128485 3.373293 3.196914 ## 7808 7859 7900 7917 7929 7977 8010 8055 ## 2.564555 2.457774 2.776710 3.185064 2.783985 2.626024 2.649542 2.811661 ## 8143 8145 8157 8195 8196 8208 8213 8214 ## 3.040923 3.409586 3.270035 3.141863 3.133301 3.039365 2.923953 2.895535 ## 8226 8231 8254 8263 8266 8317 8319 8326 ## 3.576747 2.896649 2.782438 3.055945 3.270551 3.179643 3.313609 3.620846 ## 8333 8336 8338 8346 8407 8511 8523 8560 ## 3.480284 2.632379 2.926105 3.273351 3.462364 3.248004 3.006084 2.999161 ## 8574 8578 8648 8676 8727 8792 8800 8850 ## 2.785021 2.748091 2.861330 2.950619 2.878199 2.792013 2.642290 2.860954 ## 8912 8968 8997 9010 9082 9123 9152 9161 ## 2.632838 2.886228 2.997329 2.681401 2.439120 2.790073 2.970414 3.518632 ## 9209 9210 9219 9226 9285 9317 9319 9348 ## 3.029675 3.164715 2.532392 3.090050 3.263125 3.090179 3.335531 3.517874 ## 9350 9360 9384 9396 9398 9496 9514 9529 ## 2.747039 2.747103 3.196415 3.048000 3.025001 3.219398 2.934031 3.517669 ## 9601 9616 9636 9638 9688 9733 9735 9815 ## 2.478605 3.308452 2.493620 3.135583 2.871998 2.622413 2.555936 3.197328 ## 9823 9844 9934 9980 10001 10034 10096 10108 ## 2.923922 2.886228 2.710209 2.642857 3.286879 3.176896 3.320412 3.150498 ## 10197 10210 10217 10220 10233 10276 10287 10350 ## 3.414480 2.442025 3.415007 3.020489 3.090124 2.626143 2.862338 3.305753 ## 10351 10386 10412 10552 10630 10709 10715 10727 ## 2.585180 2.657856 3.019542 3.026912 2.790073 2.782602 3.008182 3.083998 ## 10782 10800 10886 10903 10909 10917 10921 10980 ## 2.469040 3.257361 3.180766 2.523731 2.860268 2.449059 3.425870 3.257464 ## 11026 11078 11091 11204 11279 11402 11406 11435 ## 3.406395 3.019779 2.981300 2.713518 3.184448 2.729435 2.811661 2.570225 ## 11454 11474 11487 11503 11507 11598 11605 11610 ## 2.743368 2.514423 2.273232 2.660127 2.806852 2.678122 2.719207 2.782849 ## 11683 11729 11795 11806 11839 11846 11896 11924 ## 3.112563 2.746924 2.844411 2.994090 3.140131 2.800299 2.602132 2.988201 ## 11930 11958 12083 12086 12089 12101 12143 12146 ## 2.351741 3.017568 2.263834 3.083077 3.274528 2.704883 2.599324 2.809138 ## 12150 12208 12271 12287 12288 12365 12419 12429 ## 3.287253 3.353175 2.790101 2.466267 2.946758 3.106844 2.790383 3.425624 ## 12433 12475 12539 12578 12607 12711 12733 12752 ## 2.420893 2.832867 2.476729 2.878963 2.695290 2.901157 2.330046 2.573646 ## 12774 12786 12833 12871 12876 12886 12957 12976 ## 2.662935 2.599986 2.904128 3.055031 2.641126 2.807372 3.040229 2.755559 ## 13016 13017 13022 13029 13046 13052 13057 13077 ## 3.417180 3.476312 3.568416 2.837878 2.755559 3.102465 3.113267 3.114496 ## 13092 13105 13112 13122 13220 13221 13243 13310 ## 3.050248 2.845574 3.155015 3.332366 2.430184 2.276895 2.925396 2.747174 ## 13356 13358 13359 13366 13474 13510 13548 13568 ## 3.093096 2.618345 2.623750 2.479811 3.021040 2.539788 2.430929 2.764956 ## 13573 13584 13590 13600 13601 13616 13659 13708 ## 2.534431 2.645527 3.011021 2.962510 2.492657 2.730789 2.650109 2.868645 ## 13718 13782 13837 13896 13913 13917 13932 13933 ## 3.061893 2.792621 2.772830 3.346548 2.598853 3.381887 2.471366 2.724714 ## 13978 14018 14050 14072 14098 14162 14176 14198 ## 2.600259 2.880745 3.664472 3.033902 3.191092 2.865511 2.952953 2.860268 ## 14258 14259 14294 14295 14326 14367 14376 14399 ## 2.875225 2.469983 3.249871 2.713518 2.594471 2.810564 2.783779 3.182974 ## 14412 14465 14499 14512 14592 14644 14654 14683 ## 3.190560 2.980949 3.505541 3.690583 2.542768 2.817335 2.304618 2.807372 ## 14725 14741 14828 14829 14886 14940 14946 14954 ## 2.969227 2.985458 2.697326 2.466737 3.145866 2.356618 3.282041 3.008760 ## 15015 15101 15104 15114 15287 15289 15324 15355 ## 2.568758 2.769849 2.555234 3.244725 2.799118 3.220308 3.556590 3.276802 ## 15366 15469 15481 15590 15594 15628 15681 15735 ## 2.723132 2.638708 2.705936 3.100770 2.381933 2.935394 3.216137 3.239776 ## 15749 15750 15754 15758 15760 15765 15777 15785 ## 3.384704 3.516188 3.514095 3.520652 3.212966 3.608636 3.437759 3.152509 ## 15786 15791 15805 15815 15821 15827 15830 15880 ## 3.481646 3.255376 2.700494 3.265136 3.348101 3.517139 3.187060 3.110830 ## 15891 15902 15934 15941 15952 15961 15965 15971 ## 3.193349 3.714094 3.169789 3.232084 3.315654 3.356737 3.562094 3.122832 ## 15989 16035 16042 16053 16054 16056 16071 16091 ## 3.299678 3.221284 3.014416 3.464375 3.473961 2.927407 3.159375 3.555265 ## 16097 16111 16173 16258 16267 16293 16328 16405 ## 3.351173 3.480937 2.595507 2.787767 2.793084 2.646590 2.607116 2.906384 ## 16454 16502 16515 16516 16526 16590 16592 16632 ## 3.358797 2.575218 2.770689 2.647454 2.977852 2.779557 2.878467 3.526109 ## 16649 16650 16665 16672 16673 16676 16699 16757 ## 2.773134 2.863806 3.199719 3.331058 3.464375 3.401603 2.882098 3.551680 ## 16822 16844 16853 16886 16914 16952 16963 16971 ## 3.474671 2.735219 3.099418 3.248663 3.107956 2.815627 2.601526 3.448631 ## 17039 17069 17081 17091 17132 17135 17152 17157 ## 2.416265 2.552472 2.732800 2.799740 3.195670 2.955886 2.783477 2.800747 ## 17190 17208 17222 17230 17257 17293 17294 17402 ## 2.647563 3.069653 2.777815 2.848641 2.793262 2.861168 2.686719 2.417203 ## 17455 17494 17505 17529 17544 17678 17847 17875 ## 2.834641 2.486331 2.749697 2.858687 2.984336 3.244309 3.037455 3.041415 ## 17878 17879 17959 17997 18001 18039 18066 18125 ## 2.646071 2.947250 2.751080 3.070538 3.205364 2.514498 2.847078 2.989497 ## 18158 18164 18323 18401 18433 18460 18469 18470 ## 2.697404 3.046861 2.724165 3.527250 2.932882 2.664725 3.131272 3.113342 ## 18564 18565 18618 18694 18748 18753 18767 18858 ## 3.091645 3.000447 3.342855 2.587929 2.839579 2.747013 3.050876 2.740299 ## 18960 18984 19009 19076 19089 19090 19091 19094 ## 2.873387 3.467569 2.767178 3.504871 3.191277 3.139801 2.822440 3.502355 ## 19121 19122 19173 19192 19195 19208 19223 19434 ## 3.303374 3.374685 3.339694 2.567525 2.330253 2.800747 2.740299 2.885277 ## 19478 19492 19556 19594 19616 19726 19730 19733 ## 3.445357 2.951892 2.795562 3.032479 3.056944 3.074790 2.941744 3.173019 ## 19757 19801 19827 19847 19858 19869 19875 19906 ## 2.972071 3.380165 2.860108 2.424384 2.441589 2.232110 2.599261 2.692920 ## 19994 19995 20008 20015 20031 20049 20056 20083 ## 2.831725 2.407047 2.657506 3.148543 2.896903 2.791363 2.905944 2.664401 ## 20107 20111 20127 20163 20200 20241 20258 20267 ## 2.795667 2.490694 3.146094 2.563411 2.396161 2.596753 3.466035 3.678169 ## 20385 20427 20443 20520 20605 20679 20767 20806 ## 2.386165 2.836262 2.631594 2.804303 3.008397 2.924797 3.008649 2.891907 ## 20837 20875 20898 20903 20905 20912 20925 20937 ## 3.077707 2.797785 3.133210 2.929868 2.744293 2.625647 3.369021 2.727499 ## 21080 21085 21165 21176 21265 21266 21269 21284 ## 3.022307 2.580516 2.879550 2.826243 3.207662 3.283998 3.108848 2.691858 ## 21304 21333 21337 21365 21398 21418 21542 21559 ## 3.163137 2.232110 2.567706 3.346551 2.729273 2.655225 2.785654 3.094746 ## 21626 21690 21779 21830 21900 21922 21987 22009 ## 3.058768 2.980862 2.905345 2.805037 3.083569 2.423860 2.597930 3.401739 ## 22150 22269 22336 22345 22377 22399 22414 22423 ## 2.308446 3.085418 3.036176 3.196270 2.620568 3.057593 2.677924 3.119933 ## 22471 22714 22972 22973 22979 23002 23007 23053 ## 2.969788 3.721346 3.126603 2.939269 2.775603 2.741388 3.094053 2.841212 ## 23144 23176 23192 23302 23320 23338 23391 23441 ## 2.867323 3.091255 3.399768 2.678750 2.563752 2.767178 2.642704 2.824370 ## 23468 23493 23517 23528 23531 23548 23553 23591 ## 3.232750 2.670633 2.681284 2.775603 2.549311 3.300897 2.907745 3.688897 ## 23681 23701 23780 23840 23848 23857 24022 24023 ## 2.925199 2.675506 2.624561 2.624397 2.807388 3.014823 3.073608 2.755584 ## 24030 24058 24059 24124 24135 24352 24418 24421 ## 2.389318 2.967909 3.114712 2.469926 3.150024 2.977259 2.671498 3.131272 ## 24453 24463 24470 24471 24478 24498 24527 24576 ## 2.857084 3.142006 3.009469 2.621414 3.155093 2.657787 3.065349 3.150426 ## 24593 24601 24687 24714 24741 24804 24816 24911 ## 3.050876 2.601124 2.942821 2.752929 2.629750 2.903934 2.921344 2.745706 ## 24922 24924 24947 25024 25066 25070 25085 25103 ## 2.980886 3.186868 2.833660 2.902548 2.755649 2.949469 3.376068 2.726612 ## 25127 25141 25148 25275 25356 25392 25408 25432 ## 2.413455 2.930122 2.540796 2.796418 2.794290 3.413211 2.820307 2.669043 ## 25519 25570 25571 25597 25611 25621 25652 25711 ## 2.710115 2.796976 2.723939 2.670310 2.295860 3.000743 2.380831 3.001285 ## 25728 25735 25737 25766 25804 25849 25888 25890 ## 2.820115 3.014233 3.069530 3.387951 2.851479 3.644922 2.684099 2.367786 ## 25947 25990 26078 26081 26084 26086 26120 26168 ## 2.988004 2.760290 2.731489 2.380831 2.993922 2.462162 3.502219 3.457387 ## 26314 26331 26476 26528 26602 26612 26613 26644 ## 2.712185 3.126173 2.810868 2.525219 2.586276 3.301202 3.363585 3.064085 ## 26696 26755 26801 26811 26861 26902 26916 26919 ## 2.684321 2.819399 3.331036 2.766494 3.126131 2.923047 2.706563 3.139545 ## 26956 27110 27148 27246 27395 27403 27415 27420 ## 3.223426 2.778321 3.304095 2.648727 2.767012 3.227156 2.821524 3.214151 ## 27531 27541 27555 27578 27579 27598 27600 27606 ## 3.040861 3.048308 3.385778 3.248080 3.429799 3.167417 2.690347 2.985094 ## 27611 27698 27699 27757 27764 27787 27791 27870 ## 2.992102 2.613061 2.689963 2.775728 3.542449 2.716503 2.934718 2.929703 ## 27925 27928 27945 28038 28119 28156 28157 28191 ## 2.673675 2.633052 3.180282 2.968123 3.262777 2.799444 2.465556 2.815750 ## 28237 28242 28251 28254 28360 28363 28379 28394 ## 3.008311 2.731244 2.971210 3.261576 2.949695 3.294052 2.941437 2.609867 ## 28422 28496 28572 28621 28765 28770 28776 28807 ## 2.806628 3.082127 2.723916 2.866546 2.637370 3.235074 3.037555 2.719983 ## 28886 28961 28969 28980 28997 29004 29014 29093 ## 3.413106 2.699416 2.921469 2.733039 2.716503 3.226393 2.954921 2.795414 ## 29101 29118 29159 29164 29169 29225 29275 29288 ## 2.950096 3.020908 3.478804 2.859092 2.503292 2.667893 3.026353 3.193546 ## 29294 29387 29431 29480 29508 29521 29540 29554 ## 2.900029 2.866459 2.623956 3.194736 2.802281 3.648312 3.189868 2.852089 ## 29557 29562 29569 29584 29593 29633 29634 29639 ## 3.072721 3.401966 3.397119 3.397747 3.693485 3.406763 2.722689 3.582981 ## 29652 29687 29715 29748 29830 29868 29871 29916 ## 2.782137 3.266794 3.002642 2.577031 2.901424 2.335838 2.653470 2.385368 ## 29920 29958 30006 30128 30194 30197 30263 30290 ## 2.525828 2.908758 3.364388 3.228235 2.866970 3.093340 3.138425 2.928069 ## 30369 30427 30477 30478 30491 30539 30583 30591 ## 3.217922 2.737111 3.119816 3.057985 3.440781 2.463042 3.415199 3.121343 ## 30606 30613 30624 30632 30649 30694 30727 30737 ## 3.078165 3.305130 3.272426 3.110170 2.893884 3.218469 3.158782 2.787540 ## 30747 30753 30756 30777 30783 30817 30870 30892 ## 3.328520 3.119508 3.386635 3.022257 2.444432 2.668947 3.140760 2.815947 ## 30898 30984 31002 31003 31038 31069 31132 31147 ## 3.477839 2.737057 2.906667 3.413590 3.377261 2.962718 3.528125 2.866060 ## 31183 31184 31191 31222 31254 31324 31363 31391 ## 2.739399 2.779588 3.217096 2.640962 3.085992 3.392725 3.207045 2.789176 ## 31396 31426 31431 31433 31504 31525 31615 31667 ## 2.961050 3.263336 3.207624 2.875036 3.236769 3.617178 3.542060 3.295834 ## 31668 31689 31758 31768 31863 31867 31868 31897 ## 3.389677 3.177946 3.550217 2.836428 3.403739 3.198997 3.305430 3.106381 ## 31899 31902 31956 31969 31975 31982 31997 32042 ## 3.063874 2.847226 3.342601 3.210594 3.279223 2.680762 3.406409 2.788163 ## 32045 32074 32087 32095 32143 32152 32212 32229 ## 2.505339 3.085367 3.027446 3.046780 2.684815 2.950308 3.404048 2.979846 ## 32256 32314 32357 32369 32458 32502 32504 32517 ## 3.075402 3.566477 3.167155 2.795414 2.851532 2.943600 2.752415 3.184073 ## 32561 32564 32571 32591 32599 32626 ## 3.481612 2.347673 2.475821 2.982382 2.551125 3.497855 # lwage_test = test[&quot;lwage&quot;].values # test = sm.add_constant(test) #add constant # # lwage_pred = basic_results.predict(test) # predict out of sample # print(lwage_pred) Finally, we test the predictions. R code y.test &lt;- log(test$wage) MSE.test1 &lt;- sum((y.test-trainregbasic)^2)/length(y.test) R2.test1&lt;- 1- MSE.test1/var(y.test) Test MSE for the basic model: 0.1971044. Test R2 for the basic model: 0.3279559. Python code # MSE_test1 = np.sum((lwage_test-lwage_pred)**2)/len(lwage_test) # R2_test1 = 1 - MSE_test1/np.var(lwage_test) # # print(&quot;Test MSE for the basic model: &quot;, MSE_test1, &quot; &quot;) # print(&quot;Test R2 for the basic model: &quot;, R2_test1) In the basic model, the \\(MSE test\\) is quite closed to the \\(MSE sample\\). # estimating the parameters regflex &lt;- lm(flex, data=train) # calculating the out-of-sample MSE trainregflex&lt;- predict(regflex, newdata=test) y.test &lt;- log(test$wage) MSE.test2 &lt;- sum((y.test-trainregflex)^2)/length(y.test) R2.test2&lt;- 1- MSE.test2/var(y.test) Test MSE for the flexible model: 0.2064107. Test R2 for the flexible model: 0.2962252. # # Flexible model # # estimating the parameters in the training sample # flex_results = smf.ols(flex , data=train).fit() # # # calculating the out-of-sample MSE # lwage_flex_pred = flex_results.predict(test) # predict out of sample # lwage_test = test[&quot;lwage&quot;].values # # MSE_test2 = np.sum((lwage_test-lwage_flex_pred)**2)/len(lwage_test) # R2_test2 = 1 - MSE_test2/np.var(lwage_test) # # print(&quot;Test MSE for the flexible model: &quot;, MSE_test2, &quot; &quot;) # print(&quot;Test R2 for the flexible model: &quot;, R2_test2) In the flexible model, the discrepancy between the \\(MSE_{test}\\) and the \\(MSE_{sample}\\) is not large. It is worth to notice that the \\(MSE_{test}\\) vary across different data splits. Hence, it is a good idea average the out-of-sample MSE over different data splits to get valid results. Nevertheless, we observe that, based on the out-of-sample \\(MSE\\), the basic model using OLS regression performs is about as well (or slightly better) than the flexible model. Next, let us use lasso regression in the flexible model instead of OLS regression. Lasso (least absolute shrinkage and selection operator) is a penalized regression method that can be used to reduce the complexity of a regression model when the number of regressors \\(p\\) is relatively large in relation to \\(n\\). Note that the out-of-sample \\(MSE\\) on the test sample can be computed for any other black-box prediction method as well. Thus, let us finally compare the performance of lasso regression in the flexible model to OLS regression. R code # flexible model using lasso # estimating the parameters reglasso &lt;- rlasso(flex, data=train, post=FALSE) # calculating the out-of-sample MSE trainreglasso&lt;- predict(reglasso, newdata=test) MSE.lasso &lt;- sum((y.test-trainreglasso)^2)/length(y.test) R2.lasso&lt;- 1- MSE.lasso/var(y.test) Test \\(MSE\\) for the lasso on flexible model: 0.212698 Test \\(R^2\\) for the lasso flexible model: 0.2747882 Python code # # flexible model using lasso # # get exogenous variables from training data used in flex model # flex_results_0 = smf.ols(flex , data=train) # X_train = flex_results_0.exog # print(X_train.shape) # # # Get endogenous variable # lwage_train = train[&quot;lwage&quot;] # print(lwage_train.shape) # # calculating the out-of-sample MSE # reg = linear_model.Lasso(alpha=0.1) # lwage_lasso_fitted = reg.fit(X_train, lwage_train).predict( X_test ) # # MSE_lasso = np.sum((lwage_test-lwage_lasso_fitted)**2)/len(lwage_test) # R2_lasso = 1 - MSE_lasso/np.var(lwage_test) # # print(&quot;Test MSE for the flexible model: &quot;, MSE_lasso, &quot; &quot;) # print(&quot;Test R2 for the flexible model: &quot;, R2_lasso) Finally, let us summarize the results: R code Models &lt;- c(&quot;Basic regression&quot;,&quot;Flexible regression&quot;,&quot;Lasso regression&quot;) R_2_SAMPLE &lt;- c(R2.test1,R2.test2,R2.lasso) MSE_SAMPLE&lt;- c(MSE.test1,MSE.test2,MSE.lasso) data.frame(Models,R_2_SAMPLE,MSE_SAMPLE) %&gt;% kable(&quot;markdown&quot;,caption = &quot;Descriptive Statistics - Random Process&quot;) Table 1.4: Descriptive Statistics - Random Process Models R_2_SAMPLE MSE_SAMPLE Basic regression 0.3279559 0.1971044 Flexible regression 0.2962252 0.2064107 Lasso regression 0.2747882 0.2126980 print(data.frame(Models,R_2,MSE),type=&quot;latex&quot;) ## Models R_2 MSE ## 1 Basic regression 0.3100465 0.2244251 ## 2 Flexible regression 0.3511099 0.2110681 ## 3 Lasso regression 0.2778653 0.2348928 Python code # Package for latex table # import array_to_latex as a2l # # table2 = np.zeros((3, 2)) # table2[0,0] = MSE_test1 # table2[1,0] = MSE_test2 # table2[2,0] = MSE_lasso # table2[0,1] = R2_test1 # table2[1,1] = R2_test2 # table2[2,1] = R2_lasso # # table2 = pd.DataFrame(table2, columns = [&quot;$MSE_{test}$&quot;, &quot;$R^2_{test}$&quot;], \\ # index = [&quot;basic reg&quot;,&quot;flexible reg&quot;,&quot;lasso regression&quot;]) # table2 # table2.to_latex # print(table2.to_latex()) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
