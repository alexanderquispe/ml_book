[["testing-the-convergence-hypothesis.html", "Chapter 4 Testing the Convergence Hypothesis 4.1 Introduction 4.2 Data analysis", " Chapter 4 Testing the Convergence Hypothesis 4.1 Introduction We provide an additional empirical example of partialling-out with Lasso to estimate the regression coefficient \\(\\beta_1\\) in the high-dimensional linear regression model: \\[ Y = \\beta_1 D + \\beta_2&#39;W + \\epsilon. \\] Specifically, we are interested in how the rates at which economies of different countries grow (\\(Y\\)) are related to the initial wealth levels in each country (\\(D\\)) controlling for countrys institutional, educational, and other similar characteristics (\\(W\\)). The relationship is captured by \\(\\beta_1\\), the speed of convergence/divergence, which measures the speed at which poor countries catch up \\((\\beta_1&lt; 0)\\) or fall behind \\((\\beta_1&gt; 0)\\) rich countries, after controlling for \\(W\\). Our inference question here is: do poor countries grow faster than rich countries, controlling for educational and other characteristics? In other words, is the speed of convergence negative: $ _1 &lt;0?$ This is the Convergence Hypothesis predicted by the Solow Growth Model. This is a structural economic model. Under some strong assumptions, that we wont state here, the predictive exercise we are doing here can be given causal interpretation. The outcome \\(Y\\) is the realized annual growth rate of a countrys wealth (Gross Domestic Product per capita). The target regressor (\\(D\\)) is the initial level of the countrys wealth. The target parameter \\(\\beta_1\\) is the speed of convergence, which measures the speed at which poor countries catch up with rich countries. The controls (\\(W\\)) include measures of education levels, quality of institutions, trade openness, and political stability in the country. .col2 { columns: 2 200px; /* number of columns and width in pixels*/ -webkit-columns: 2 200px; /* chrome, safari */ -moz-columns: 2 200px; /* firefox */ } .col3 { columns: 3 100px; -webkit-columns: 3 100px; -moz-columns: 3 100px; } 4.2 Data analysis We consider the data set GrowthData which is included in the package hdm. First, let us load the data set to get familiar with the data. library(hdm) library(xtable) import pandas as pd import numpy as np import pyreadr import statsmodels.api as sm import statsmodels.formula.api as smf Import data growth &lt;- GrowthData attach(growth) # check variables names(growth) ## [1] &quot;Outcome&quot; &quot;intercept&quot; &quot;gdpsh465&quot; &quot;bmp1l&quot; &quot;freeop&quot; &quot;freetar&quot; ## [7] &quot;h65&quot; &quot;hm65&quot; &quot;hf65&quot; &quot;p65&quot; &quot;pm65&quot; &quot;pf65&quot; ## [13] &quot;s65&quot; &quot;sm65&quot; &quot;sf65&quot; &quot;fert65&quot; &quot;mort65&quot; &quot;lifee065&quot; ## [19] &quot;gpop1&quot; &quot;fert1&quot; &quot;mort1&quot; &quot;invsh41&quot; &quot;geetot1&quot; &quot;geerec1&quot; ## [25] &quot;gde1&quot; &quot;govwb1&quot; &quot;govsh41&quot; &quot;gvxdxe41&quot; &quot;high65&quot; &quot;highm65&quot; ## [31] &quot;highf65&quot; &quot;highc65&quot; &quot;highcm65&quot; &quot;highcf65&quot; &quot;human65&quot; &quot;humanm65&quot; ## [37] &quot;humanf65&quot; &quot;hyr65&quot; &quot;hyrm65&quot; &quot;hyrf65&quot; &quot;no65&quot; &quot;nom65&quot; ## [43] &quot;nof65&quot; &quot;pinstab1&quot; &quot;pop65&quot; &quot;worker65&quot; &quot;pop1565&quot; &quot;pop6565&quot; ## [49] &quot;sec65&quot; &quot;secm65&quot; &quot;secf65&quot; &quot;secc65&quot; &quot;seccm65&quot; &quot;seccf65&quot; ## [55] &quot;syr65&quot; &quot;syrm65&quot; &quot;syrf65&quot; &quot;teapri65&quot; &quot;teasec65&quot; &quot;ex1&quot; ## [61] &quot;im1&quot; &quot;xr65&quot; &quot;tot1&quot; # I downloaded the data that the author used growth_read = pyreadr.read_r(&quot;./data/GrowthData.RData&quot;) # Extracting the data frame from rdata_read growth = growth_read[ &#39;GrowthData&#39; ] # check variables list(growth) ## [&#39;Outcome&#39;, &#39;intercept&#39;, &#39;gdpsh465&#39;, &#39;bmp1l&#39;, &#39;freeop&#39;, &#39;freetar&#39;, &#39;h65&#39;, &#39;hm65&#39;, &#39;hf65&#39;, &#39;p65&#39;, &#39;pm65&#39;, &#39;pf65&#39;, &#39;s65&#39;, &#39;sm65&#39;, &#39;sf65&#39;, &#39;fert65&#39;, &#39;mort65&#39;, &#39;lifee065&#39;, &#39;gpop1&#39;, &#39;fert1&#39;, &#39;mort1&#39;, &#39;invsh41&#39;, &#39;geetot1&#39;, &#39;geerec1&#39;, &#39;gde1&#39;, &#39;govwb1&#39;, &#39;govsh41&#39;, &#39;gvxdxe41&#39;, &#39;high65&#39;, &#39;highm65&#39;, &#39;highf65&#39;, &#39;highc65&#39;, &#39;highcm65&#39;, &#39;highcf65&#39;, &#39;human65&#39;, &#39;humanm65&#39;, &#39;humanf65&#39;, &#39;hyr65&#39;, &#39;hyrm65&#39;, &#39;hyrf65&#39;, &#39;no65&#39;, &#39;nom65&#39;, &#39;nof65&#39;, &#39;pinstab1&#39;, &#39;pop65&#39;, &#39;worker65&#39;, &#39;pop1565&#39;, &#39;pop6565&#39;, &#39;sec65&#39;, &#39;secm65&#39;, &#39;secf65&#39;, &#39;secc65&#39;, &#39;seccm65&#39;, &#39;seccf65&#39;, &#39;syr65&#39;, &#39;syrm65&#39;, &#39;syrf65&#39;, &#39;teapri65&#39;, &#39;teasec65&#39;, &#39;ex1&#39;, &#39;im1&#39;, &#39;xr65&#39;, &#39;tot1&#39;] We determine the dimension of our data set. dim(growth) ## [1] 90 63 growth.shape ## (90, 63) 4.2.1 OLS The sample contains \\(90\\) countries and \\(63\\) controls. Thus \\(p \\approx 60\\), \\(n=90\\) and \\(p/n\\) is not small. We expect the least squares method to provide a poor estimate of \\(\\beta_1\\). We expect the method based on partialling-out with Lasso to provide a high quality estimate of \\(\\beta_1\\). To check this hypothesis, we analyze the relation between the output variable \\(Y\\) and the other countrys characteristics by running a linear regression in the first step. reg.ols &lt;- lm(Outcome~.-1,data=growth) # We create the main variables y = growth[&#39;Outcome&#39;] X = growth.drop(&#39;Outcome&#39;, 1) # OLS regression reg_ols = sm.OLS(y, X).fit() We determine the regression coefficient \\(\\beta_1\\) of the target regressor gdpsh465 (\\(D\\)), its 95% confidence interval and the standard error. # output: estimated regression coefficient corresponding to the target regressor est_ols &lt;- summary(reg.ols)$coef[&quot;gdpsh465&quot;,1] # output: std. error std_ols &lt;- summary(reg.ols)$coef[&quot;gdpsh465&quot;,2] # output: 95% confidence interval ci_ols &lt;- confint(reg.ols)[2,] results_ols &lt;- as.data.frame(cbind(est_ols,std_ols,ci_ols[1],ci_ols[2])) colnames(results_ols) &lt;-c(&quot;estimator&quot;,&quot;standard error&quot;, &quot;lower bound CI&quot;, &quot;upper bound CI&quot;) rownames(results_ols) &lt;-c(&quot;OLS&quot;) # output: estimated regression coefficient corresponding to the target regressor est_ols = reg_ols.summary2().tables[1][&#39;Coef.&#39;][&#39;gdpsh465&#39;] # output: std. error std_ols = reg_ols.summary2().tables[1][&#39;Std.Err.&#39;][&#39;gdpsh465&#39;] # output: 95% confidence interval lower_ci = reg_ols.summary2().tables[1][&#39;[0.025&#39;][&#39;gdpsh465&#39;] upper_ci = reg_ols.summary2().tables[1][&#39;0.975]&#39;][&#39;gdpsh465&#39;] table &lt;- matrix(0, 1, 4) table[1,1:4] &lt;- c(est_ols,std_ols,ci_ols[1],ci_ols[2]) colnames(table) &lt;-c(&quot;estimator&quot;,&quot;standard error&quot;, &quot;lower bound CI&quot;, &quot;upper bound CI&quot;) rownames(table) &lt;-c(&quot;OLS&quot;) tab&lt;- xtable(table, digits = 3) print(tab,type=&quot;html&quot;) # set type=&quot;latex&quot; for printing table in LaTeX ## &lt;!-- html table generated in R 4.0.4 by xtable 1.8-4 package --&gt; ## &lt;!-- Mon Sep 27 14:52:04 2021 --&gt; ## &lt;table border=1&gt; ## &lt;tr&gt; &lt;th&gt; &lt;/th&gt; &lt;th&gt; estimator &lt;/th&gt; &lt;th&gt; standard error &lt;/th&gt; &lt;th&gt; lower bound CI &lt;/th&gt; &lt;th&gt; upper bound CI &lt;/th&gt; &lt;/tr&gt; ## &lt;tr&gt; &lt;td align=&quot;right&quot;&gt; OLS &lt;/td&gt; &lt;td align=&quot;right&quot;&gt; -0.009 &lt;/td&gt; &lt;td align=&quot;right&quot;&gt; 0.030 &lt;/td&gt; &lt;td align=&quot;right&quot;&gt; -0.071 &lt;/td&gt; &lt;td align=&quot;right&quot;&gt; 0.052 &lt;/td&gt; &lt;/tr&gt; ## &lt;/table&gt; table_1 = np.zeros( (1, 4) ) table_1[0,0] = est_ols table_1[0,1] = std_ols table_1[0,2] = lower_ci table_1[0,3] = upper_ci table_1_pandas = pd.DataFrame( table_1, columns = [ &quot;Estimator&quot;,&quot;Std. Error&quot;, &quot;lower bound CI&quot;, &quot;upper bound CI&quot; ]) table_1_pandas.index = [ &quot;OLS&quot; ] table_1_html = table_1_pandas.to_html() table_1_html ## &#39;&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;\\n &lt;thead&gt;\\n &lt;tr style=&quot;text-align: right;&quot;&gt;\\n &lt;th&gt;&lt;/th&gt;\\n &lt;th&gt;Estimator&lt;/th&gt;\\n &lt;th&gt;Std. Error&lt;/th&gt;\\n &lt;th&gt;lower bound CI&lt;/th&gt;\\n &lt;th&gt;upper bound CI&lt;/th&gt;\\n &lt;/tr&gt;\\n &lt;/thead&gt;\\n &lt;tbody&gt;\\n &lt;tr&gt;\\n &lt;th&gt;OLS&lt;/th&gt;\\n &lt;td&gt;-0.009378&lt;/td&gt;\\n &lt;td&gt;0.029888&lt;/td&gt;\\n &lt;td&gt;-0.0706&lt;/td&gt;\\n &lt;td&gt;0.051844&lt;/td&gt;\\n &lt;/tr&gt;\\n &lt;/tbody&gt;\\n&lt;/table&gt;&#39; estimator standard error lower bound CI upper bound CI OLS -0.009 0.030 -0.071 0.052 4.2.2 Lasso Least squares provides a rather noisy estimate (high standard error) of the speed of convergence, and does not allow us to answer the question about the convergence hypothesis since the confidence interval includes zero. In contrast, we can use the partialling-out approach based on lasso regression (Double Lasso). Y &lt;- growth[, 1, drop = F] # output variable W &lt;- as.matrix(growth)[, -c(1, 2,3)] # controls D &lt;- growth[, 3, drop = F] # target regressor Y = growth[&#39;Outcome&#39;] W = growth.drop([&#39;Outcome&#39;,&#39;intercept&#39;, &#39;gdpsh465&#39;], 1 ) D = growth[&#39;gdpsh465&#39;] we run the regression using lasso r.Y &lt;- rlasso(x=W,y=Y)$res # creates the &quot;residual&quot; output variable r.D &lt;- rlasso(x=W,y=D)$res # creates the &quot;residual&quot; target regressor partial.lasso &lt;- lm(r.Y ~ r.D) est_lasso &lt;- partial.lasso$coef[2] std_lasso &lt;- summary(partial.lasso)$coef[2,2] ci_lasso &lt;- confint(partial.lasso)[2,] from sklearn import linear_model # Seat values for Lasso lasso_model = linear_model.Lasso( alpha = 0.00077 ) r_Y = Y - lasso_model.fit( W, Y ).predict( W ) ## C:\\Users\\MSI-NB\\ANACON~1\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.0058288944773880885, tolerance: 2.3434976975716032e-05 ## model = cd_fast.enet_coordinate_descent( r_Y = r_Y.rename(&#39;r_Y&#39;) r_D = D - lasso_model.fit( W, D ).predict( W ) # Part. out d ## C:\\Users\\MSI-NB\\ANACON~1\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.99724470007767, tolerance: 0.007147912790119585 ## model = cd_fast.enet_coordinate_descent( r_D = r_D.rename(&#39;r_D&#39;) partial_lasso_fit = sm.OLS(r_Y, r_D).fit() # Regress residuales est_lasso = partial_lasso_fit.summary2().tables[1][&#39;Coef.&#39;][&#39;r_D&#39;] std_lasso = partial_lasso_fit.summary2().tables[1][&#39;Std.Err.&#39;][&#39;r_D&#39;] lower_ci_lasso = partial_lasso_fit.summary2().tables[1][&#39;[0.025&#39;][&#39;r_D&#39;] upper_ci_lasso = partial_lasso_fit.summary2().tables[1][&#39;0.975]&#39;][&#39;r_D&#39;] now we export results into a table table &lt;- matrix(0, 1, 4) table[1,1:4] &lt;- c(est_lasso,std_lasso,ci_lasso[1],ci_lasso[2]) colnames(table) &lt;-c(&quot;estimator&quot;,&quot;standard error&quot;, &quot;lower bound CI&quot;, &quot;upper bound CI&quot;) rownames(table) &lt;-c(&quot;Double Lasso&quot;) tab&lt;- xtable(table, digits = 3) print(tab,type=&quot;html&quot;) # set type=&quot;latex&quot; for printing table in LaTeX table_2 = np.zeros( (1, 4) ) table_2[0,0] = est_lasso table_2[0,1] = std_lasso table_2[0,2] = lower_ci_lasso table_2[0,3] = upper_ci_lasso table_2_pandas = pd.DataFrame( table_2, columns = [ &quot;Estimator&quot;,&quot;Std. Error&quot;, &quot;lower bound CI&quot;, &quot;upper bound CI&quot; ]) table_2_pandas.index = [ &quot;LASSO&quot; ] table_2_pandas estimator standard error lower bound CI upper bound CI Double Lasso -0.050 0.014 -0.078 -0.022 Lasso provides a more precise estimate (lower standard error). The Lasso based point estimate is about \\(5\\%\\) and the \\(95\\%\\) confidence interval for the (annual) rate of convergence is \\(7.8\\%\\) to \\(2.2\\%\\). This empirical evidence does support the convergence hypothesis. Note: Alternatively, one could also use the rlassoEffect funtion from the hdm package that directly applies the partialling-out approach. lasso.effect = rlassoEffect(x = W, y = Y, d = D, method = &quot;partialling out&quot;) lasso.effect print(&quot;no package in python&quot;) 4.2.3 Summary results Finally, let us have a look at the results. table &lt;- matrix(0, 2, 4) table[1,1:4] &lt;- c(est_ols,std_ols,ci_ols[1],ci_ols[2]) table[2,1:4] &lt;- c(est_lasso,std_lasso,ci_lasso[1],ci_lasso[2]) colnames(table) &lt;-c(&quot;estimator&quot;,&quot;standard error&quot;, &quot;lower bound CI&quot;, &quot;upper bound CI&quot;) rownames(table) &lt;-c(&quot;OLS&quot;,&quot;Double Lasso&quot;) tab&lt;- xtable(table, digits = 3) # print(tab,type=&quot;html&quot;) # set type=&quot;latex&quot; for printing table in LaTeX table ## estimator standard error lower bound CI upper bound CI ## OLS -0.009377989 0.02988773 -0.07060022 0.05184424 ## Double Lasso -0.049811465 0.01393636 -0.07750705 -0.02211588 table_2 = np.zeros( (1, 4) ) table_2[0,0] = est_lasso table_2[0,1] = std_lasso table_2[0,2] = lower_ci_lasso table_2[0,3] = upper_ci_lasso table_2_pandas = pd.DataFrame( table_2, columns = [ &quot;Estimator&quot;,&quot;Std. Error&quot;, &quot;lower bound CI&quot;, &quot;upper bound CI&quot; ]) table_2_pandas.index = [ &quot;Double LASSO&quot; ] table_3 = table_1_pandas.append(table_2_pandas) table_3 ## Estimator Std. Error lower bound CI upper bound CI ## OLS -0.009378 0.029888 -0.070600 0.051844 ## Double LASSO -0.047747 0.017705 -0.082926 -0.012567 The least square method provides a rather noisy estimate of the speed of convergence. We can not answer the question if poor countries grow faster than rich countries. The least square method does not work when the ratio \\(p/n\\) is large. In sharp contrast, partialling-out via Lasso provides a more precise estimate. The Lasso based point estimate is \\(-5\\%\\) and the \\(95\\%\\) confidence interval for the (annual) rate of convergence \\([-7.8\\%,-2.2\\%]\\) only includes negative numbers. This empirical evidence does support the convergence hypothesis. estimator standard error lower bound CI upper bound CI OLS -0.009 0.030 -0.071 0.052 Double Lasso -0.050 0.014 -0.078 -0.022 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
