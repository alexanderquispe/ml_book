# Penalized Linear Regressions: A Simulation Experiment

## Data Generating Process: Approximately Sparse

1. Import libraries

:::::: {.columns}
::: {.column width="49.5%" data-latex="{0.48\textwidth}"}

**R code**
```{r libraries-r,echo= TRUE,message=FALSE,warning=FALSE}
# No libraries requiered
```
:::
::: {.column width="1%" data-latex="{0.04\textwidth}"}
\ 
<!-- an empty Div (with a white space), serving as
a column separator -->
:::
:::::: {.column width="49.5%" data-latex="{0.48\textwidth}"}

**Python code**
```{python libraries-python, message=FALSE,warning=FALSE}
import random
import numpy as np
import math
import matplotlib.pyplot as plt
```
:::
::::::

2. To set seed

:::::: {.columns}
::: {.column width="49.5%" data-latex="{0.48\textwidth}"}

**R code**
```{r seed-r,echo= TRUE,message=FALSE,warning=FALSE}
set.seed(1)
n = 100
p = 400
```
:::
::: {.column width="1%" data-latex="{0.04\textwidth}"}
\ 
<!-- an empty Div (with a white space), serving as
a column separator -->
:::
:::::: {.column width="49.5%" data-latex="{0.48\textwidth}"}

**Python code**
```{python seed-python, message=FALSE,warning=FALSE}
random.seed(1)
n = 100
p = 400
```
:::
::::::

3. To create variables

:::::: {.columns}
::: {.column width="49.5%" data-latex="{0.48\textwidth}"}

```{r variables-r,echo= TRUE,message=FALSE,warning=FALSE}
Z= runif(n)-1/2
W = matrix(runif(n*p)-1/2, n, p)
```
:::
::: {.column width="1%" data-latex="{0.04\textwidth}"}
\ 
<!-- an empty Div (with a white space), serving as
a column separator -->
:::
:::::: {.column width="49.5%" data-latex="{0.48\textwidth}"}

```{python variables-python, message=FALSE,warning=FALSE}
Z = np.random.uniform( low = 0 , high = 1 , size = n) - 1/2 
W = ( np.random.uniform( low = 0 , high = 1 , size = n * p ) - 1/2 ).\
        reshape( n , p )
```
:::
::::::

:::::: {.columns}
::: {.column width="49.5%" data-latex="{0.48\textwidth}"}

```{r variables-two-r,echo= TRUE,message=FALSE,warning=FALSE}
beta = 1/seq(1:p)^2   # approximately sparse beta
gX = exp(4*Z)+ W%*%beta  # leading term nonlinear
X = cbind(Z, Z^2, Z^3, W )  # polynomials in Zs will be approximating exp(4*Z)
```
:::
::: {.column width="1%" data-latex="{0.04\textwidth}"}
\ 
<!-- an empty Div (with a white space), serving as
a column separator -->
:::
:::::: {.column width="49.5%" data-latex="{0.48\textwidth}"}

```{python variables-two-python, message=FALSE,warning=FALSE}
beta = ((1/ np.arange(1, p + 1 )) ** 2)
gX = np.exp( 4 * Z ) + (W @ beta)
X = np.concatenate( ( Z.reshape(Z.size, 1), Z.reshape(Z.size, 1) \
                     ** 2, Z.reshape(Z.size, 1) ** 3, W ) , axis = 1 )
```
:::
::::::

4. Generate $Y$

:::::: {.columns}
::: {.column width="49.5%" data-latex="{0.48\textwidth}"}

```{r y-r, message=FALSE,warning=FALSE}
Y = gX + rnorm(n)   #generate Y
```

:::
::: {.column width="1%" data-latex="{0.04\textwidth}"}
\ 
<!-- an empty Div (with a white space), serving as
a column separator -->
:::
:::::: {.column width="49.5%" data-latex="{0.48\textwidth}"}

```{python y-python, message=FALSE,warning=FALSE}
mean = 0
sd = 1
Y = gX + np.random.normal( mean , sd, n )
```
:::
::::::

:::::: {.columns}
::: {.column width="49.5%" data-latex="{0.48\textwidth}"}

```{r plot-r, message=FALSE,warning=FALSE}
#
#
#
#
# 
plot(gX,Y, xlab="g(X)", ylab="Y", title("Y vs g(X)")) 
```

:::
::: {.column width="1%" data-latex="{0.04\textwidth}"}
\ 
<!-- an empty Div (with a white space), serving as
a column separator -->
:::
:::::: {.column width="49.5%" data-latex="{0.48\textwidth}"}

```{python plot-python, message=FALSE,warning=FALSE}
fig = plt.figure()
fig.suptitle('Y vs g(X)')
ax = fig.add_subplot(111)
plt.scatter( Y, gX)
plt.xlabel('g(X)')
plt.ylabel('Y')
plt.show()
```
:::
::::::

:::::: {.columns}
::: {.column width="49.5%" data-latex="{0.48\textwidth}"}

```{r parameter-r, message=FALSE,warning=FALSE}
print( c("theoretical R2:", var(gX)/var(Y)))
dim(X)
```

:::
::: {.column width="1%" data-latex="{0.04\textwidth}"}
\ 
<!-- an empty Div (with a white space), serving as
a column separator -->
:::
:::::: {.column width="49.5%" data-latex="{0.48\textwidth}"}

```{python parameter-python, message=FALSE,warning=FALSE}
print( f"theoretical R2:, {np.var(gX) / np.var( Y )}" ) 
X.shape
```
:::
::::::

We use package Glmnet to carry out predictions using cross-validated lasso, ridge, and elastic net.

We should know that **cv.glmnet** function in r **standarize** ***X*** data by default. So, we have to standarize our data before the execution of sklearn package. The **normalize** parameter will help for this. However, the function cv.glamnet  is also standarizing the **Y** [variable](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html) and then unstadarize the coefficients from the regression. To do this with sklearn, we will standarize the **Y** variable before fitting with **StandardScaler** function. Finally, the r-function uses 10 folds by default so we will adjust our model to use $cv=10$ ten folds.\
\
The parameter $l1_{ratio}$ corresponds to **alpha** in the glmnet R package while **alpha** corresponds to the **lambda** parameter in **glmnet**. Specifically, $l1_{ratio} = 1$ is the lasso penalty. Currently, $l1_{ratio} <= 0.01$ is not reliable, unless you supply your own sequence of **alpha**.

:::::: {.columns}
::: {.column width="49.5%" data-latex="{0.48\textwidth}"}

```{r lib-r, message=FALSE,warning=FALSE}
library(glmnet)
```

:::
::: {.column width="1%" data-latex="{0.04\textwidth}"}
\ 
<!-- an empty Div (with a white space), serving as
a column separator -->
:::
:::::: {.column width="49.5%" data-latex="{0.48\textwidth}"}

```{python lib-python, message=FALSE,warning=FALSE}
from sklearn.linear_model import LassoCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import RidgeCV, ElasticNetCV
import statsmodels.api as sm
```
:::
::::::

:::::: {.columns}
::: {.column width="49.5%" data-latex="{0.48\textwidth}"}

```{r estimation-r, message=FALSE,warning=FALSE}
fit.lasso.cv   <- cv.glmnet(X, Y, family="gaussian", alpha=1)  # family gaussian means that we'll be using square loss
fit.ridge   <- cv.glmnet(X, Y, family="gaussian", alpha=0)     # family gaussian means that we'll be using square loss
fit.elnet   <- cv.glmnet(X, Y, family="gaussian", alpha=.5)    # family gaussian means that we'll be using square loss

yhat.lasso.cv    <- predict(fit.lasso.cv, newx = X)            # predictions
yhat.ridge   <- predict(fit.ridge, newx = X)
yhat.elnet   <- predict(fit.elnet, newx = X)

MSE.lasso.cv <- summary(lm((gX-yhat.lasso.cv)^2~1))$coef[1:2]  # report MSE and standard error for MSE for approximating g(X)
MSE.ridge <- summary(lm((gX-yhat.ridge)^2~1))$coef[1:2]        # report MSE and standard error for MSE for approximating g(X)
MSE.elnet <- summary(lm((gX-yhat.elnet)^2~1))$coef[1:2]        # report MSE and standard error for MSE for approximating g(X)
```

:::
::: {.column width="1%" data-latex="{0.04\textwidth}"}
\ 
<!-- an empty Div (with a white space), serving as
a column separator -->
:::
:::::: {.column width="49.5%" data-latex="{0.48\textwidth}"}

```{python estimation-python, message=FALSE,warning=FALSE}
# Reshaping Y variable
Y_vec = Y.reshape( Y.size, 1)

# Scalar distribution
scaler = StandardScaler()
scaler.fit( Y_vec )
std_Y = scaler.transform( Y_vec )

# Regressions
fit_lasso_cv = LassoCV(cv = 10 , random_state = 0 , normalize = True ).fit( X, std_Y )
fit_ridge = ElasticNetCV( cv = 10 , normalize = True , random_state = 0 , l1_ratio = 0.0001 ).fit( X, std_Y )
fit_elnet = ElasticNetCV( cv = 10 , normalize = True , random_state = 0 , l1_ratio = 0.5, max_iter = 100000 ).fit( X, std_Y )

# Predictions
yhat_lasso_cv = scaler.inverse_transform( fit_lasso_cv.predict( X ) )
yhat_ridge = scaler.inverse_transform( fit_ridge.predict( X ) )
yhat_elnet = scaler.inverse_transform( fit_elnet.predict( X ) )

MSE_lasso_cv = sm.OLS( ((gX - yhat_lasso_cv)**2 ) , np.ones( yhat_lasso_cv.shape )  ).fit().summary2().tables[1].round(3)
MSE_ridge = sm.OLS( ((gX - yhat_ridge)**2 ) , np.ones( yhat_ridge.size )  ).fit().summary2().tables[1].round(3)
MSE_elnet = sm.OLS( ((gX - yhat_elnet)**2 ) , np.ones( yhat_elnet.size )  ).fit().summary2().tables[1].round(3)
# our coefficient of MSE_elnet are far from r output
```
:::
::::::

Here we compute the lasso and ols post lasso using plug-in choices for penalty levels, using package hdm.

Rlasso functionality: it is searching the right set of regressors. This function was made for the case of ***p*** regressors and ***n*** observations where ***p >>>> n***. It assumes that the error is i.i.d. The errors may be non-Gaussian or heteroscedastic.

The post lasso function makes OLS with the selected ***T*** regressors.
To select those parameters, they use $\lambda$ as variable to penalize **Funny thing: the function rlasso was named like that because it is the "rigorous" Lasso.**
We find a Python code that tries to replicate the main function of hdm r-package. I was made by [Max Huppertz](https://maxhuppertz.github.io/code/). 

His library is this [repository](https://github.com/maxhuppertz/hdmpy). Download its repository and copy this folder to your site-packages folder. In my case it is located here ***C:\Python\Python38\Lib\site-packages*** .

We need to install this package ***pip install multiprocess***.